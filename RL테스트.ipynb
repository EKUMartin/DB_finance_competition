{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8afb251f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu=12, n_envs=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:155: UserWarning: You have specified a mini-batch size of 256, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2046`, after every 7 untruncated mini-batches, there will be a truncated mini-batch of size 254\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=341 and n_envs=6)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for timesteps=200,000 (window=30, assets=10, ~5y data)\n",
      "[speed]    673.1 steps/s | elapsed=    3.0s | timesteps=2,046 | est_steps_in_4h≈9,691,982\n",
      "[speed]    430.3 steps/s | elapsed=    7.8s | timesteps=4,092 | est_steps_in_4h≈6,195,807\n",
      "[speed]    369.3 steps/s | elapsed=   13.3s | timesteps=6,138 | est_steps_in_4h≈5,318,231\n",
      "[speed]    464.0 steps/s | elapsed=   17.7s | timesteps=8,184 | est_steps_in_4h≈6,681,735\n",
      "[speed]    472.4 steps/s | elapsed=   22.1s | timesteps=10,230 | est_steps_in_4h≈6,802,584\n",
      "[speed]    551.1 steps/s | elapsed=   25.8s | timesteps=12,276 | est_steps_in_4h≈7,935,254\n",
      "[speed]    554.9 steps/s | elapsed=   29.5s | timesteps=14,322 | est_steps_in_4h≈7,990,577\n",
      "[speed]    547.0 steps/s | elapsed=   33.2s | timesteps=16,368 | est_steps_in_4h≈7,876,612\n",
      "[speed]    566.6 steps/s | elapsed=   36.8s | timesteps=18,414 | est_steps_in_4h≈8,159,430\n",
      "[speed]    554.4 steps/s | elapsed=   40.5s | timesteps=20,460 | est_steps_in_4h≈7,983,927\n",
      "[speed]    563.6 steps/s | elapsed=   44.1s | timesteps=22,506 | est_steps_in_4h≈8,116,350\n",
      "[speed]    564.0 steps/s | elapsed=   47.8s | timesteps=24,552 | est_steps_in_4h≈8,121,737\n",
      "[speed]    564.0 steps/s | elapsed=   51.4s | timesteps=26,598 | est_steps_in_4h≈8,121,482\n",
      "[speed]    561.8 steps/s | elapsed=   55.0s | timesteps=28,644 | est_steps_in_4h≈8,090,637\n",
      "[speed]    543.6 steps/s | elapsed=   58.8s | timesteps=30,690 | est_steps_in_4h≈7,827,629\n",
      "[speed]    566.9 steps/s | elapsed=   62.4s | timesteps=32,736 | est_steps_in_4h≈8,163,258\n",
      "[speed]    573.2 steps/s | elapsed=   66.0s | timesteps=34,782 | est_steps_in_4h≈8,253,618\n",
      "[speed]    572.2 steps/s | elapsed=   69.6s | timesteps=36,828 | est_steps_in_4h≈8,240,268\n",
      "[speed]    403.7 steps/s | elapsed=   74.6s | timesteps=38,874 | est_steps_in_4h≈5,813,625\n",
      "[speed]    384.6 steps/s | elapsed=   79.9s | timesteps=40,920 | est_steps_in_4h≈5,538,635\n",
      "[speed]    433.3 steps/s | elapsed=   84.7s | timesteps=42,966 | est_steps_in_4h≈6,239,016\n",
      "[speed]    445.6 steps/s | elapsed=   89.3s | timesteps=45,012 | est_steps_in_4h≈6,416,865\n",
      "[speed]    454.0 steps/s | elapsed=   93.8s | timesteps=47,058 | est_steps_in_4h≈6,537,470\n",
      "[speed]    464.8 steps/s | elapsed=   98.2s | timesteps=49,104 | est_steps_in_4h≈6,692,706\n",
      "[speed]    477.1 steps/s | elapsed=  102.5s | timesteps=51,150 | est_steps_in_4h≈6,869,646\n",
      "[speed]    475.8 steps/s | elapsed=  106.8s | timesteps=53,196 | est_steps_in_4h≈6,851,021\n",
      "[speed]    481.5 steps/s | elapsed=  111.0s | timesteps=55,242 | est_steps_in_4h≈6,933,448\n",
      "[speed]    342.2 steps/s | elapsed=  117.0s | timesteps=57,288 | est_steps_in_4h≈4,927,860\n",
      "[speed]    419.8 steps/s | elapsed=  121.9s | timesteps=59,334 | est_steps_in_4h≈6,045,243\n",
      "[speed]    491.9 steps/s | elapsed=  126.0s | timesteps=61,380 | est_steps_in_4h≈7,082,691\n",
      "[speed]    483.3 steps/s | elapsed=  130.3s | timesteps=63,426 | est_steps_in_4h≈6,960,172\n",
      "[speed]    490.3 steps/s | elapsed=  134.4s | timesteps=65,472 | est_steps_in_4h≈7,060,731\n",
      "[speed]    516.0 steps/s | elapsed=  138.4s | timesteps=67,518 | est_steps_in_4h≈7,430,696\n",
      "[speed]    465.3 steps/s | elapsed=  142.8s | timesteps=69,564 | est_steps_in_4h≈6,700,405\n",
      "[speed]    486.4 steps/s | elapsed=  147.0s | timesteps=71,610 | est_steps_in_4h≈7,004,513\n",
      "[speed]    456.1 steps/s | elapsed=  151.5s | timesteps=73,656 | est_steps_in_4h≈6,567,756\n",
      "[speed]    493.9 steps/s | elapsed=  155.6s | timesteps=75,702 | est_steps_in_4h≈7,111,536\n",
      "[speed]    487.4 steps/s | elapsed=  159.8s | timesteps=77,748 | est_steps_in_4h≈7,018,227\n",
      "[speed]    498.2 steps/s | elapsed=  163.9s | timesteps=79,794 | est_steps_in_4h≈7,174,463\n",
      "[speed]    508.6 steps/s | elapsed=  168.0s | timesteps=81,840 | est_steps_in_4h≈7,324,223\n",
      "[speed]    495.9 steps/s | elapsed=  172.1s | timesteps=83,886 | est_steps_in_4h≈7,141,659\n",
      "[speed]    505.3 steps/s | elapsed=  176.1s | timesteps=85,932 | est_steps_in_4h≈7,276,544\n",
      "[speed]    490.1 steps/s | elapsed=  180.3s | timesteps=87,978 | est_steps_in_4h≈7,057,642\n",
      "[speed]    471.9 steps/s | elapsed=  184.6s | timesteps=90,024 | est_steps_in_4h≈6,795,573\n",
      "[speed]    478.0 steps/s | elapsed=  188.9s | timesteps=92,070 | est_steps_in_4h≈6,883,164\n",
      "[speed]    478.6 steps/s | elapsed=  193.2s | timesteps=94,116 | est_steps_in_4h≈6,891,164\n",
      "[speed]    486.1 steps/s | elapsed=  197.4s | timesteps=96,162 | est_steps_in_4h≈7,000,351\n",
      "[speed]    482.1 steps/s | elapsed=  201.6s | timesteps=98,208 | est_steps_in_4h≈6,941,903\n",
      "[speed]    476.2 steps/s | elapsed=  205.9s | timesteps=100,254 | est_steps_in_4h≈6,856,901\n",
      "[speed]    485.2 steps/s | elapsed=  210.2s | timesteps=102,300 | est_steps_in_4h≈6,987,076\n",
      "[speed]    457.5 steps/s | elapsed=  214.6s | timesteps=104,346 | est_steps_in_4h≈6,587,972\n",
      "[speed]    456.2 steps/s | elapsed=  219.1s | timesteps=106,392 | est_steps_in_4h≈6,569,125\n",
      "[speed]    532.5 steps/s | elapsed=  223.0s | timesteps=108,438 | est_steps_in_4h≈7,667,472\n",
      "[speed]    489.2 steps/s | elapsed=  227.1s | timesteps=110,484 | est_steps_in_4h≈7,044,315\n",
      "[speed]    418.3 steps/s | elapsed=  232.0s | timesteps=112,530 | est_steps_in_4h≈6,022,830\n",
      "[speed]    451.8 steps/s | elapsed=  236.6s | timesteps=114,576 | est_steps_in_4h≈6,506,037\n",
      "[speed]    446.6 steps/s | elapsed=  241.1s | timesteps=116,622 | est_steps_in_4h≈6,431,593\n",
      "[speed]    461.4 steps/s | elapsed=  245.6s | timesteps=118,668 | est_steps_in_4h≈6,643,612\n",
      "[speed]    424.4 steps/s | elapsed=  250.4s | timesteps=120,714 | est_steps_in_4h≈6,111,345\n",
      "[speed]    443.0 steps/s | elapsed=  255.0s | timesteps=122,760 | est_steps_in_4h≈6,378,772\n",
      "[speed]    446.2 steps/s | elapsed=  259.6s | timesteps=124,806 | est_steps_in_4h≈6,424,767\n",
      "[speed]    498.3 steps/s | elapsed=  263.7s | timesteps=126,852 | est_steps_in_4h≈7,175,458\n",
      "[speed]    502.5 steps/s | elapsed=  267.8s | timesteps=128,898 | est_steps_in_4h≈7,235,573\n",
      "[speed]    500.2 steps/s | elapsed=  271.9s | timesteps=130,944 | est_steps_in_4h≈7,203,448\n",
      "[speed]    451.5 steps/s | elapsed=  276.4s | timesteps=132,990 | est_steps_in_4h≈6,501,565\n",
      "[speed]    493.7 steps/s | elapsed=  280.5s | timesteps=135,036 | est_steps_in_4h≈7,109,322\n",
      "[speed]    467.2 steps/s | elapsed=  284.9s | timesteps=137,082 | est_steps_in_4h≈6,727,153\n",
      "[speed]    499.9 steps/s | elapsed=  289.0s | timesteps=139,128 | est_steps_in_4h≈7,198,426\n",
      "[speed]    505.3 steps/s | elapsed=  293.1s | timesteps=141,174 | est_steps_in_4h≈7,276,689\n",
      "[speed]    489.5 steps/s | elapsed=  297.2s | timesteps=143,220 | est_steps_in_4h≈7,048,940\n",
      "[speed]    521.0 steps/s | elapsed=  301.2s | timesteps=145,266 | est_steps_in_4h≈7,501,906\n",
      "[speed]    509.6 steps/s | elapsed=  305.2s | timesteps=147,312 | est_steps_in_4h≈7,338,794\n",
      "[speed]    504.2 steps/s | elapsed=  309.2s | timesteps=149,358 | est_steps_in_4h≈7,260,877\n",
      "[speed]    501.7 steps/s | elapsed=  313.3s | timesteps=151,404 | est_steps_in_4h≈7,224,159\n",
      "[speed]    479.6 steps/s | elapsed=  317.6s | timesteps=153,450 | est_steps_in_4h≈6,905,529\n",
      "[speed]    500.4 steps/s | elapsed=  321.7s | timesteps=155,496 | est_steps_in_4h≈7,205,726\n",
      "[speed]    509.3 steps/s | elapsed=  325.7s | timesteps=157,542 | est_steps_in_4h≈7,334,003\n",
      "[speed]    505.3 steps/s | elapsed=  329.7s | timesteps=159,588 | est_steps_in_4h≈7,276,812\n",
      "[speed]    483.4 steps/s | elapsed=  334.0s | timesteps=161,634 | est_steps_in_4h≈6,960,796\n",
      "[speed]    488.7 steps/s | elapsed=  338.2s | timesteps=163,680 | est_steps_in_4h≈7,036,706\n",
      "[speed]    496.0 steps/s | elapsed=  342.3s | timesteps=165,726 | est_steps_in_4h≈7,142,615\n",
      "[speed]    517.5 steps/s | elapsed=  346.2s | timesteps=167,772 | est_steps_in_4h≈7,452,182\n",
      "[speed]    375.4 steps/s | elapsed=  351.7s | timesteps=169,818 | est_steps_in_4h≈5,406,275\n",
      "[speed]    373.0 steps/s | elapsed=  357.2s | timesteps=171,864 | est_steps_in_4h≈5,371,587\n",
      "[speed]    431.2 steps/s | elapsed=  361.9s | timesteps=173,910 | est_steps_in_4h≈6,209,189\n",
      "[speed]    419.6 steps/s | elapsed=  366.8s | timesteps=175,956 | est_steps_in_4h≈6,042,195\n",
      "[speed]    488.8 steps/s | elapsed=  371.0s | timesteps=178,002 | est_steps_in_4h≈7,038,600\n",
      "[speed]    540.4 steps/s | elapsed=  374.8s | timesteps=180,048 | est_steps_in_4h≈7,782,134\n",
      "[speed]    465.4 steps/s | elapsed=  379.2s | timesteps=182,094 | est_steps_in_4h≈6,701,303\n",
      "[speed]    544.7 steps/s | elapsed=  382.9s | timesteps=184,140 | est_steps_in_4h≈7,842,999\n",
      "[speed]    557.5 steps/s | elapsed=  386.6s | timesteps=186,186 | est_steps_in_4h≈8,027,341\n",
      "[speed]    560.9 steps/s | elapsed=  390.2s | timesteps=188,232 | est_steps_in_4h≈8,077,026\n",
      "[speed]    562.2 steps/s | elapsed=  393.9s | timesteps=190,278 | est_steps_in_4h≈8,095,625\n",
      "[speed]    559.4 steps/s | elapsed=  397.5s | timesteps=192,324 | est_steps_in_4h≈8,055,621\n",
      "[speed]    563.8 steps/s | elapsed=  401.2s | timesteps=194,370 | est_steps_in_4h≈8,119,219\n",
      "[speed]    564.3 steps/s | elapsed=  404.8s | timesteps=196,416 | est_steps_in_4h≈8,126,483\n",
      "[speed]    563.8 steps/s | elapsed=  408.4s | timesteps=198,462 | est_steps_in_4h≈8,118,646\n",
      "[speed]    569.9 steps/s | elapsed=  412.0s | timesteps=200,508 | est_steps_in_4h≈8,206,998\n",
      "\n",
      "DONE: total_time=6.91 min, avg_speed=482.6 steps/s\n",
      "EST: in 4 hours you can do about 6,949,579 timesteps at this speed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray, axis=-1) -> np.ndarray:\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / (np.sum(e, axis=axis, keepdims=True) + 1e-12)\n",
    "\n",
    "\n",
    "def make_synthetic_prices(T: int, N: int, seed: int = 0) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    간단한 랜덤워크(로그수익률)로 가격 생성 (벤치마크용).\n",
    "    실제 데이터로 바꾸려면 여기만 교체하면 됨.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # 일간 로그수익률: 평균 0, 표준편차 1% 정도 (대충)\n",
    "    log_rets = rng.normal(loc=0.0, scale=0.01, size=(T, N)).astype(np.float32)\n",
    "    prices = np.exp(np.cumsum(log_rets, axis=0)).astype(np.float32)\n",
    "    # 시작값 100으로 스케일\n",
    "    prices = prices * 100.0\n",
    "    return prices\n",
    "\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    관측:\n",
    "      - seq: 최근 window일의 \"수익률\" (shape: [window, N])\n",
    "      - static: 직전 포트폴리오 비중(N) + 포트폴리오 가치(1) (shape: [N+1])\n",
    "    행동:\n",
    "      - raw action (shape [N]) -> softmax로 비중으로 변환 (합=1)\n",
    "    보상:\n",
    "      - log(1 + 포트수익) - 거래비용\n",
    "    \"\"\"\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        prices: np.ndarray,\n",
    "        window: int = 30,\n",
    "        cost_rate: float = 0.001,  # 거래비용(턴오버 페널티 계수)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert prices.ndim == 2, \"prices must be (T, N)\"\n",
    "        self.prices = prices\n",
    "        self.T, self.N = prices.shape\n",
    "        self.window = window\n",
    "        self.cost_rate = cost_rate\n",
    "\n",
    "        # 일간 단순수익률 r_t = P_t / P_{t-1} - 1\n",
    "        self.returns = (prices[1:] / prices[:-1] - 1.0).astype(np.float32)  # shape (T-1, N)\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"seq\": spaces.Box(low=-np.inf, high=np.inf, shape=(window, self.N), dtype=np.float32),\n",
    "                \"static\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.N + 1,), dtype=np.float32),\n",
    "            }\n",
    "        )\n",
    "        self.action_space = spaces.Box(low=-5.0, high=5.0, shape=(self.N,), dtype=np.float32)\n",
    "\n",
    "        self._t = None\n",
    "        self._w_prev = None\n",
    "        self._value = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._t = self.window  # returns 인덱스 기준\n",
    "        self._w_prev = np.ones(self.N, dtype=np.float32) / self.N\n",
    "        self._value = 1.0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # seq: 최근 window일 수익률 (returns는 (T-1, N))\n",
    "        seq = self.returns[self._t - self.window : self._t]  # (window, N)\n",
    "        static = np.concatenate([self._w_prev, np.array([self._value], dtype=np.float32)], axis=0)\n",
    "        return {\"seq\": seq.astype(np.float32), \"static\": static.astype(np.float32)}\n",
    "\n",
    "    def step(self, action):\n",
    "        # action -> weights\n",
    "        w = softmax(action.astype(np.float32), axis=0).astype(np.float32)\n",
    "\n",
    "        # 거래비용(턴오버)\n",
    "        turnover = np.sum(np.abs(w - self._w_prev)).astype(np.float32)\n",
    "        cost = self.cost_rate * turnover\n",
    "\n",
    "        # 다음날(현재 t)의 수익률 적용\n",
    "        # returns[t]는 (P_{t+1}/P_t - 1)에 해당하는 느낌이라,\n",
    "        # 여기서는 단순히 returns[self._t]를 \"오늘->내일\" 수익으로 사용\n",
    "        r = self.returns[self._t]  # (N,)\n",
    "        port_ret = float(np.dot(w, r))\n",
    "\n",
    "        reward = float(np.log1p(port_ret) - cost)\n",
    "\n",
    "        # 가치 업데이트(단순)\n",
    "        self._value = float(self._value * (1.0 + port_ret - cost))\n",
    "        self._w_prev = w\n",
    "\n",
    "        self._t += 1\n",
    "        terminated = (self._t >= (self.returns.shape[0] - 1))\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs() if not terminated else self._get_obs()  # 종료 시에도 형태 유지\n",
    "        info = {\"portfolio_value\": self._value, \"turnover\": float(turnover), \"cost\": float(cost), \"port_ret\": port_ret}\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class FusionExtractor(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    Dict 관측에서\n",
    "      - seq: LSTM으로 요약\n",
    "      - static: MLP로 요약\n",
    "    둘을 concat해서 feature로 반환\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: spaces.Dict, lstm_hidden: int = 64, static_hidden: int = 32):\n",
    "        # features_dim은 내부에서 계산해서 super에 전달\n",
    "        self.seq_shape = observation_space[\"seq\"].shape  # (window, N)\n",
    "        self.static_dim = observation_space[\"static\"].shape[0]  # N+1\n",
    "        window, n_assets = self.seq_shape\n",
    "\n",
    "        # 임시로 feature dim 계산\n",
    "        features_dim = lstm_hidden + static_hidden\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_assets,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.static_mlp = nn.Sequential(\n",
    "            nn.Linear(self.static_dim, static_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # observations[\"seq\"]: (batch, window, N)\n",
    "        seq = observations[\"seq\"]\n",
    "        static = observations[\"static\"]\n",
    "\n",
    "        # LSTM output: out (batch, window, hidden), take last time step\n",
    "        out, _ = self.lstm(seq)\n",
    "        lstm_feat = out[:, -1, :]  # (batch, hidden)\n",
    "\n",
    "        static_feat = self.static_mlp(static)  # (batch, static_hidden)\n",
    "        fused = th.cat([lstm_feat, static_feat], dim=1)\n",
    "        return fused\n",
    "\n",
    "\n",
    "class SpeedCallback(BaseCallback):\n",
    "    def __init__(self, report_every_rollout=True):\n",
    "        super().__init__()\n",
    "        self.report_every_rollout = report_every_rollout\n",
    "        self.t0 = None\n",
    "        self.last_t = None\n",
    "        self.last_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.t0 = time.time()\n",
    "        self.last_t = self.t0\n",
    "        self.last_steps = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 학습 계속 진행하라는 의미로 True 반환\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        if not self.report_every_rollout:\n",
    "            return\n",
    "        now = time.time()\n",
    "        steps = self.model.num_timesteps\n",
    "        dt = now - self.last_t\n",
    "        ds = steps - self.last_steps\n",
    "        sps = ds / max(dt, 1e-9)\n",
    "        possible_4h = sps * 4 * 3600\n",
    "\n",
    "        print(\n",
    "            f\"[speed] {sps:8.1f} steps/s | \"\n",
    "            f\"elapsed={now - self.t0:7.1f}s | \"\n",
    "            f\"timesteps={steps:,} | \"\n",
    "            f\"est_steps_in_4h≈{int(possible_4h):,}\"\n",
    "        )\n",
    "\n",
    "        self.last_t = now\n",
    "        self.last_steps = steps\n",
    "\n",
    "\n",
    "\n",
    "def make_env(prices, window, cost_rate, seed):\n",
    "    def _init():\n",
    "        env = PortfolioEnv(prices=prices, window=window, cost_rate=cost_rate)\n",
    "        env.reset(seed=seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ===== 사용자 조건 =====\n",
    "    N_ASSETS = 10\n",
    "    WINDOW = 30\n",
    "    # 5년 일봉 거래일 대략 252*5 = 1260 (대충)\n",
    "    T_DAYS = 252 * 5 + 1  # prices 길이 (returns는 -1)\n",
    "    COST_RATE = 0.001\n",
    "\n",
    "    # ===== 데이터 (벤치마크: synthetic) =====\n",
    "    prices = make_synthetic_prices(T=T_DAYS, N=N_ASSETS, seed=42)\n",
    "\n",
    "    # ===== 병렬 환경 수 =====\n",
    "    cpu = os.cpu_count() or 8\n",
    "    n_envs = max(1, min(8, cpu // 2))  # 너무 과하게 늘리면 오히려 느릴 수 있음\n",
    "    print(f\"cpu={cpu}, n_envs={n_envs}\")\n",
    "\n",
    "    # Windows에서 SubprocVecEnv 문제 생기면 DummyVecEnv로 바꿔서 테스트 가능\n",
    "    use_subproc = True\n",
    "\n",
    "    env_fns = [make_env(prices, WINDOW, COST_RATE, seed=1000 + i) for i in range(n_envs)]\n",
    "    vec_env = SubprocVecEnv(env_fns) if use_subproc else DummyVecEnv(env_fns)\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=FusionExtractor,\n",
    "        features_extractor_kwargs=dict(lstm_hidden=64, static_hidden=32),\n",
    "        net_arch=[128, 128],  # fusion 이후의 policy/value MLP\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        env=vec_env,\n",
    "        verbose=0,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        n_steps=2048 // n_envs if n_envs > 1 else 2048,  # rollout 길이\n",
    "        batch_size=256,\n",
    "        learning_rate=3e-4,\n",
    "        n_epochs=10,\n",
    "        device=\"auto\",  # GPU 있으면 자동 사용\n",
    "    )\n",
    "\n",
    "    # ===== 테스트 러닝 =====\n",
    "    # 여기 timesteps를 바꿔가며 측정하면 됨.\n",
    "    TIMESTEPS = 200_000  # 먼저 20만으로 step/s 확인 추천\n",
    "    print(f\"Training for timesteps={TIMESTEPS:,} (window={WINDOW}, assets={N_ASSETS}, ~5y data)\")\n",
    "    cb = SpeedCallback()\n",
    "\n",
    "    t_start = time.time()\n",
    "    model.learn(total_timesteps=TIMESTEPS, callback=cb)\n",
    "    t_end = time.time()\n",
    "\n",
    "    total_sec = t_end - t_start\n",
    "    sps_total = TIMESTEPS / max(total_sec, 1e-9)\n",
    "    print(f\"\\nDONE: total_time={total_sec/60:.2f} min, avg_speed={sps_total:.1f} steps/s\")\n",
    "\n",
    "    # 4시간(14400초) 기준으로 가능한 steps\n",
    "    est_4h = sps_total * 4 * 3600\n",
    "    print(f\"EST: in 4 hours you can do about {int(est_4h):,} timesteps at this speed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff58e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assets=11, window=30\n",
      "Train episode_steps≈32220 (target~32220)\n",
      "Test  episode_steps≈2777 (target~2777)\n",
      "\n",
      "Training for timesteps=200,000 ...\n",
      "[speed]    341.7 steps/s | elapsed=    6.0s | timesteps=2,048\n",
      "[speed]    262.1 steps/s | elapsed=   13.8s | timesteps=4,096\n",
      "[speed]    294.8 steps/s | elapsed=   20.8s | timesteps=6,144\n",
      "[speed]    299.9 steps/s | elapsed=   27.6s | timesteps=8,192\n",
      "[speed]    235.0 steps/s | elapsed=   36.3s | timesteps=10,240\n",
      "[speed]    195.9 steps/s | elapsed=   46.8s | timesteps=12,288\n",
      "[speed]    177.8 steps/s | elapsed=   58.3s | timesteps=14,336\n",
      "[speed]    307.0 steps/s | elapsed=   64.9s | timesteps=16,384\n",
      "[speed]    284.9 steps/s | elapsed=   72.1s | timesteps=18,432\n",
      "[speed]    290.9 steps/s | elapsed=   79.2s | timesteps=20,480\n",
      "[speed]    300.1 steps/s | elapsed=   86.0s | timesteps=22,528\n",
      "[speed]    316.0 steps/s | elapsed=   92.5s | timesteps=24,576\n",
      "[speed]    306.5 steps/s | elapsed=   99.2s | timesteps=26,624\n",
      "[speed]    329.9 steps/s | elapsed=  105.4s | timesteps=28,672\n",
      "[speed]    330.2 steps/s | elapsed=  111.6s | timesteps=30,720\n",
      "[speed]    333.5 steps/s | elapsed=  117.7s | timesteps=32,768\n",
      "[speed]    335.3 steps/s | elapsed=  123.8s | timesteps=34,816\n",
      "[speed]    322.9 steps/s | elapsed=  130.2s | timesteps=36,864\n",
      "[speed]    317.8 steps/s | elapsed=  136.6s | timesteps=38,912\n",
      "[speed]    323.7 steps/s | elapsed=  142.9s | timesteps=40,960\n",
      "[speed]    332.0 steps/s | elapsed=  149.1s | timesteps=43,008\n",
      "[speed]    330.9 steps/s | elapsed=  155.3s | timesteps=45,056\n",
      "[speed]    274.9 steps/s | elapsed=  162.7s | timesteps=47,104\n",
      "[speed]    304.6 steps/s | elapsed=  169.5s | timesteps=49,152\n",
      "[speed]    337.2 steps/s | elapsed=  175.5s | timesteps=51,200\n",
      "[speed]    342.6 steps/s | elapsed=  181.5s | timesteps=53,248\n",
      "[speed]    329.4 steps/s | elapsed=  187.7s | timesteps=55,296\n",
      "[speed]    276.9 steps/s | elapsed=  195.1s | timesteps=57,344\n",
      "[speed]    334.1 steps/s | elapsed=  201.3s | timesteps=59,392\n",
      "[speed]    337.8 steps/s | elapsed=  207.3s | timesteps=61,440\n",
      "[speed]    337.3 steps/s | elapsed=  213.4s | timesteps=63,488\n",
      "[speed]    327.9 steps/s | elapsed=  219.6s | timesteps=65,536\n",
      "[speed]    308.0 steps/s | elapsed=  226.3s | timesteps=67,584\n",
      "[speed]    299.9 steps/s | elapsed=  233.1s | timesteps=69,632\n",
      "[speed]    331.3 steps/s | elapsed=  239.3s | timesteps=71,680\n",
      "[speed]    327.0 steps/s | elapsed=  245.6s | timesteps=73,728\n",
      "[speed]    278.6 steps/s | elapsed=  252.9s | timesteps=75,776\n",
      "[speed]    301.9 steps/s | elapsed=  259.7s | timesteps=77,824\n",
      "[speed]    319.0 steps/s | elapsed=  266.1s | timesteps=79,872\n",
      "[speed]    316.9 steps/s | elapsed=  272.6s | timesteps=81,920\n",
      "[speed]    331.2 steps/s | elapsed=  278.8s | timesteps=83,968\n",
      "[speed]    313.4 steps/s | elapsed=  285.3s | timesteps=86,016\n",
      "[speed]    335.2 steps/s | elapsed=  291.4s | timesteps=88,064\n",
      "[speed]    328.7 steps/s | elapsed=  297.6s | timesteps=90,112\n",
      "[speed]    336.3 steps/s | elapsed=  303.7s | timesteps=92,160\n",
      "[speed]    253.3 steps/s | elapsed=  311.8s | timesteps=94,208\n",
      "[speed]    326.8 steps/s | elapsed=  318.1s | timesteps=96,256\n",
      "[speed]    327.3 steps/s | elapsed=  324.3s | timesteps=98,304\n",
      "[speed]    330.1 steps/s | elapsed=  330.5s | timesteps=100,352\n",
      "[speed]    324.0 steps/s | elapsed=  336.9s | timesteps=102,400\n",
      "[speed]    269.0 steps/s | elapsed=  344.5s | timesteps=104,448\n",
      "[speed]    329.6 steps/s | elapsed=  350.7s | timesteps=106,496\n",
      "[speed]    331.5 steps/s | elapsed=  356.9s | timesteps=108,544\n",
      "[speed]    332.0 steps/s | elapsed=  363.0s | timesteps=110,592\n",
      "[speed]    331.8 steps/s | elapsed=  369.2s | timesteps=112,640\n",
      "[speed]    298.3 steps/s | elapsed=  376.1s | timesteps=114,688\n",
      "[speed]    313.6 steps/s | elapsed=  382.6s | timesteps=116,736\n",
      "[speed]    332.6 steps/s | elapsed=  388.8s | timesteps=118,784\n",
      "[speed]    330.0 steps/s | elapsed=  395.0s | timesteps=120,832\n",
      "[speed]    325.0 steps/s | elapsed=  401.3s | timesteps=122,880\n",
      "[speed]    325.3 steps/s | elapsed=  407.6s | timesteps=124,928\n",
      "[speed]    289.6 steps/s | elapsed=  414.6s | timesteps=126,976\n",
      "[speed]    341.6 steps/s | elapsed=  420.6s | timesteps=129,024\n",
      "[speed]    336.4 steps/s | elapsed=  426.7s | timesteps=131,072\n",
      "[speed]    335.4 steps/s | elapsed=  432.8s | timesteps=133,120\n",
      "[speed]    330.2 steps/s | elapsed=  439.0s | timesteps=135,168\n",
      "[speed]    273.9 steps/s | elapsed=  446.5s | timesteps=137,216\n",
      "[speed]    343.4 steps/s | elapsed=  452.5s | timesteps=139,264\n",
      "[speed]    326.2 steps/s | elapsed=  458.7s | timesteps=141,312\n",
      "[speed]    330.4 steps/s | elapsed=  464.9s | timesteps=143,360\n",
      "[speed]    324.4 steps/s | elapsed=  471.3s | timesteps=145,408\n",
      "[speed]    298.2 steps/s | elapsed=  478.1s | timesteps=147,456\n",
      "[speed]    317.3 steps/s | elapsed=  484.6s | timesteps=149,504\n",
      "[speed]    336.2 steps/s | elapsed=  490.7s | timesteps=151,552\n",
      "[speed]    318.3 steps/s | elapsed=  497.1s | timesteps=153,600\n",
      "[speed]    324.0 steps/s | elapsed=  503.4s | timesteps=155,648\n",
      "[speed]    284.1 steps/s | elapsed=  510.6s | timesteps=157,696\n",
      "[speed]    296.4 steps/s | elapsed=  517.5s | timesteps=159,744\n",
      "[speed]    335.4 steps/s | elapsed=  523.7s | timesteps=161,792\n",
      "[speed]    335.9 steps/s | elapsed=  529.7s | timesteps=163,840\n",
      "[speed]    309.4 steps/s | elapsed=  536.4s | timesteps=165,888\n",
      "[speed]    326.8 steps/s | elapsed=  542.6s | timesteps=167,936\n",
      "[speed]    275.0 steps/s | elapsed=  550.1s | timesteps=169,984\n",
      "[speed]    284.5 steps/s | elapsed=  557.3s | timesteps=172,032\n",
      "[speed]    331.6 steps/s | elapsed=  563.5s | timesteps=174,080\n",
      "[speed]    324.6 steps/s | elapsed=  569.8s | timesteps=176,128\n",
      "[speed]    328.1 steps/s | elapsed=  576.0s | timesteps=178,176\n",
      "[speed]    321.0 steps/s | elapsed=  582.4s | timesteps=180,224\n",
      "[speed]    332.2 steps/s | elapsed=  588.6s | timesteps=182,272\n",
      "[speed]    337.3 steps/s | elapsed=  594.6s | timesteps=184,320\n",
      "[speed]    336.0 steps/s | elapsed=  600.7s | timesteps=186,368\n",
      "[speed]    310.6 steps/s | elapsed=  607.3s | timesteps=188,416\n",
      "[speed]    285.0 steps/s | elapsed=  614.5s | timesteps=190,464\n",
      "[speed]    331.0 steps/s | elapsed=  620.7s | timesteps=192,512\n",
      "[speed]    331.2 steps/s | elapsed=  626.9s | timesteps=194,560\n",
      "[speed]    336.4 steps/s | elapsed=  633.0s | timesteps=196,608\n",
      "[speed]    327.5 steps/s | elapsed=  639.2s | timesteps=198,656\n",
      "[speed]    282.7 steps/s | elapsed=  646.5s | timesteps=200,704\n",
      "\n",
      "DONE: total_time=10.81 min, avg_speed=308.4 steps/s\n",
      "EST: ~32220 steps (≈one train pass) takes about 1.74 min at this training speed.\n",
      "EST: 1,000,000 timesteps -> ~54.0 min\n",
      "EST: 5,000,000 timesteps -> ~270.2 min\n",
      "EST: 10,000,000 timesteps -> ~540.4 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "\n",
    "def softmax(x: np.ndarray, axis=-1) -> np.ndarray:\n",
    "    x = x - np.max(x, axis=axis, keepdims=True)\n",
    "    e = np.exp(x)\n",
    "    return e / (np.sum(e, axis=axis, keepdims=True) + 1e-12)\n",
    "\n",
    "\n",
    "def make_synthetic_prices(T: int, N: int, seed: int = 0) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    log_rets = rng.normal(loc=0.0, scale=0.01, size=(T, N)).astype(np.float32)\n",
    "    prices = np.exp(np.cumsum(log_rets, axis=0)).astype(np.float32) * 100.0\n",
    "    return prices\n",
    "\n",
    "\n",
    "class PortfolioEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": []}\n",
    "\n",
    "    def __init__(self, prices: np.ndarray, window: int = 30, cost_rate: float = 0.001):\n",
    "        super().__init__()\n",
    "        assert prices.ndim == 2, \"prices must be (T, N)\"\n",
    "        self.prices = prices\n",
    "        self.T, self.N = prices.shape\n",
    "        self.window = window\n",
    "        self.cost_rate = cost_rate\n",
    "\n",
    "        self.returns = (prices[1:] / prices[:-1] - 1.0).astype(np.float32)  # (T-1, N)\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"seq\": spaces.Box(low=-np.inf, high=np.inf, shape=(window, self.N), dtype=np.float32),\n",
    "                \"static\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.N + 1,), dtype=np.float32),\n",
    "            }\n",
    "        )\n",
    "        self.action_space = spaces.Box(low=-5.0, high=5.0, shape=(self.N,), dtype=np.float32)\n",
    "\n",
    "        self._t = None\n",
    "        self._w_prev = None\n",
    "        self._value = None\n",
    "\n",
    "        # 훈련 구간에서 한 에피소드가 대략 몇 step인지(=데이터 한 바퀴 길이) 확인용\n",
    "        self.episode_steps = (self.returns.shape[0] - 2) - self.window\n",
    "        if self.episode_steps < 1:\n",
    "            raise ValueError(\"Not enough data for given window.\")\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self._t = self.window\n",
    "        self._w_prev = np.ones(self.N, dtype=np.float32) / self.N\n",
    "        self._value = 1.0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        seq = self.returns[self._t - self.window : self._t]\n",
    "        static = np.concatenate([self._w_prev, np.array([self._value], dtype=np.float32)], axis=0)\n",
    "        return {\"seq\": seq.astype(np.float32), \"static\": static.astype(np.float32)}\n",
    "\n",
    "    def step(self, action):\n",
    "        w = softmax(action.astype(np.float32), axis=0).astype(np.float32)\n",
    "\n",
    "        turnover = np.sum(np.abs(w - self._w_prev)).astype(np.float32)\n",
    "        cost = self.cost_rate * turnover\n",
    "\n",
    "        r = self.returns[self._t]\n",
    "        port_ret = float(np.dot(w, r))\n",
    "\n",
    "        reward = float(np.log1p(port_ret) - cost)\n",
    "\n",
    "        self._value = float(self._value * (1.0 + port_ret - cost))\n",
    "        self._w_prev = w\n",
    "\n",
    "        self._t += 1\n",
    "        terminated = (self._t >= (self.returns.shape[0] - 1))\n",
    "        truncated = False\n",
    "\n",
    "        obs = self._get_obs()\n",
    "        info = {\"portfolio_value\": self._value, \"turnover\": float(turnover), \"cost\": float(cost), \"port_ret\": port_ret}\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "class FusionExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Dict, lstm_hidden: int = 64, static_hidden: int = 32):\n",
    "        self.seq_shape = observation_space[\"seq\"].shape  # (window, N)\n",
    "        self.static_dim = observation_space[\"static\"].shape[0]\n",
    "        window, n_assets = self.seq_shape\n",
    "\n",
    "        features_dim = lstm_hidden + static_hidden\n",
    "        super().__init__(observation_space, features_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=n_assets,\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.static_mlp = nn.Sequential(\n",
    "            nn.Linear(self.static_dim, static_hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "        seq = observations[\"seq\"]       # (batch, window, N)\n",
    "        static = observations[\"static\"] # (batch, N+1)\n",
    "\n",
    "        out, _ = self.lstm(seq)\n",
    "        lstm_feat = out[:, -1, :]       # (batch, hidden)\n",
    "        static_feat = self.static_mlp(static)\n",
    "\n",
    "        return th.cat([lstm_feat, static_feat], dim=1)\n",
    "\n",
    "\n",
    "class SpeedCallback(BaseCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.t0 = None\n",
    "        self.last_t = None\n",
    "        self.last_steps = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        self.t0 = time.time()\n",
    "        self.last_t = self.t0\n",
    "        self.last_steps = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        now = time.time()\n",
    "        steps = self.model.num_timesteps\n",
    "        dt = now - self.last_t\n",
    "        ds = steps - self.last_steps\n",
    "        sps = ds / max(dt, 1e-9)\n",
    "\n",
    "        print(f\"[speed] {sps:8.1f} steps/s | elapsed={now - self.t0:7.1f}s | timesteps={steps:,}\")\n",
    "\n",
    "        self.last_t = now\n",
    "        self.last_steps = steps\n",
    "\n",
    "\n",
    "def main():\n",
    "    # ===== 표랑 “크기” 맞추기 =====\n",
    "    # Table: training data number ≈ 32220, testing ≈ 2777\n",
    "    TRAIN_POINTS = 32220\n",
    "    TEST_POINTS = 2777\n",
    "\n",
    "    # 표의 Assets: 11/11/15 중 하나\n",
    "    N_ASSETS = 11     # Crypto-A/B 맞춤 (원하면 15로 바꿔)\n",
    "    WINDOW = 30\n",
    "\n",
    "    # Env에서 유효 step(episode_steps)을 TRAIN_POINTS 수준으로 맞추려고\n",
    "    # episode_steps = (returns_len - 2) - window\n",
    "    # returns_len = prices_len - 1\n",
    "    # => prices_len ≈ TRAIN_POINTS + window + 3\n",
    "    PRICES_LEN_TRAIN = TRAIN_POINTS + WINDOW + 3\n",
    "    PRICES_LEN_TEST  = TEST_POINTS + WINDOW + 3\n",
    "\n",
    "    COST_RATE = 0.001\n",
    "\n",
    "    prices_train = make_synthetic_prices(T=PRICES_LEN_TRAIN, N=N_ASSETS, seed=42)\n",
    "    prices_test  = make_synthetic_prices(T=PRICES_LEN_TEST,  N=N_ASSETS, seed=43)\n",
    "\n",
    "    train_env = PortfolioEnv(prices=prices_train, window=WINDOW, cost_rate=COST_RATE)\n",
    "    test_env  = PortfolioEnv(prices=prices_test,  window=WINDOW, cost_rate=COST_RATE)\n",
    "\n",
    "    print(f\"Assets={N_ASSETS}, window={WINDOW}\")\n",
    "    print(f\"Train episode_steps≈{train_env.episode_steps} (target~{TRAIN_POINTS})\")\n",
    "    print(f\"Test  episode_steps≈{test_env.episode_steps} (target~{TEST_POINTS})\")\n",
    "\n",
    "    # 벤치 목적이면 env 1개로도 충분(병렬화는 오버헤드/환경에 따라 득실)\n",
    "    vec_env = DummyVecEnv([lambda: train_env])\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=FusionExtractor,\n",
    "        features_extractor_kwargs=dict(lstm_hidden=64, static_hidden=32),\n",
    "        net_arch=[128, 128],\n",
    "    )\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MultiInputPolicy\",\n",
    "        env=vec_env,\n",
    "        verbose=0,\n",
    "        policy_kwargs=policy_kwargs,\n",
    "        n_steps=2048,\n",
    "        batch_size=256,\n",
    "        learning_rate=3e-4,\n",
    "        n_epochs=10,\n",
    "        device=\"auto\",\n",
    "    )\n",
    "\n",
    "    # ===== 여기 timesteps 바꿔가며 “얼마나 걸리는지” 보면 됨 =====\n",
    "    TIMESTEPS = 200_000  # 먼저 20만으로 평균 steps/s 찍고, 1M/5M 시간 환산 추천\n",
    "    print(f\"\\nTraining for timesteps={TIMESTEPS:,} ...\")\n",
    "    cb = SpeedCallback()\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.learn(total_timesteps=TIMESTEPS, callback=cb)\n",
    "    t1 = time.time()\n",
    "\n",
    "    total_sec = t1 - t0\n",
    "    sps = TIMESTEPS / max(total_sec, 1e-9)\n",
    "\n",
    "    print(f\"\\nDONE: total_time={total_sec/60:.2f} min, avg_speed={sps:.1f} steps/s\")\n",
    "\n",
    "    # “저 정도 데이터 크기(Train_POINTS) 한 바퀴”가 학습(업데이트 포함) 기준으로 대충 몇 분인지\n",
    "    # (주의: PPO는 rollout/update가 섞여서 ‘순수 환경 스텝’만의 시간은 아님. 그래도 비교용으로는 쓸만함.)\n",
    "    est_epoch_min = (TRAIN_POINTS / max(sps, 1e-9)) / 60.0\n",
    "    print(f\"EST: ~{TRAIN_POINTS} steps (≈one train pass) takes about {est_epoch_min:.2f} min at this training speed.\")\n",
    "\n",
    "    for target in [1_000_000, 5_000_000, 10_000_000]:\n",
    "        est_min = (target / max(sps, 1e-9)) / 60.0\n",
    "        print(f\"EST: {target:,} timesteps -> ~{est_min:.1f} min\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84d3fa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "RL TRAINING ONLY (feature(LSTM) + rollout + backward + step)\n",
      "====================\n",
      "device=cpu | assets=11\n",
      "train_prices=(32253, 11) | p_rel=(32252, 11)\n",
      "window=31 horizon=32 batch=32 fee=0.0025\n",
      "warmup_iters=100 | bench_iters=2000 | target_iterations=80000\n",
      "[bench] 200/2000  speed=1.463 it/s  ETA(80k)≈15.18 h\n",
      "[bench] 400/2000  speed=1.463 it/s  ETA(80k)≈15.19 h\n",
      "[bench] 600/2000  speed=1.460 it/s  ETA(80k)≈15.22 h\n",
      "[bench] 800/2000  speed=1.459 it/s  ETA(80k)≈15.23 h\n",
      "[bench] 1000/2000  speed=1.461 it/s  ETA(80k)≈15.21 h\n",
      "[bench] 1200/2000  speed=1.462 it/s  ETA(80k)≈15.20 h\n",
      "[bench] 1400/2000  speed=1.464 it/s  ETA(80k)≈15.18 h\n",
      "[bench] 1600/2000  speed=1.464 it/s  ETA(80k)≈15.18 h\n",
      "[bench] 1800/2000  speed=1.466 it/s  ETA(80k)≈15.16 h\n",
      "[bench] 2000/2000  speed=1.467 it/s  ETA(80k)≈15.15 h\n",
      "\n",
      "====================\n",
      "RESULT (time only)\n",
      "====================\n",
      "speed = 1.467 iterations/s\n",
      "EST: 80,000 iterations -> 909.2 min (15.15 h)\n",
      "per-iter avg(ms): sample=0.19 | forward+rollout(feature포함)=265.03 | backward=415.17 | step=1.49 | total=681.88\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) Synthetic prices (PPO 때랑 동일)\n",
    "# =========================\n",
    "def make_synthetic_prices(T: int, N: int, seed: int = 0) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    log_rets = rng.normal(loc=0.0, scale=0.01, size=(T, N)).astype(np.float32)\n",
    "    prices = np.exp(np.cumsum(log_rets, axis=0)).astype(np.float32) * 100.0\n",
    "    return prices\n",
    "\n",
    "\n",
    "def price_relatives_from_prices(prices: np.ndarray) -> np.ndarray:\n",
    "    # p_t = close_t / close_{t-1}  -> (T-1, N)\n",
    "    p = prices[1:] / (prices[:-1] + 1e-12)\n",
    "    return p.astype(np.float32)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) Models: \"feature 추출(LSTM)\" + \"정책\"\n",
    "# =========================\n",
    "class Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    과거 window 수익률 -> 다음 price relative 힌트(p_hat) 생성\n",
    "    (논문 구조를 단순화해서 LSTM으로 구현)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_assets: int, hidden: int = 64):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_assets, hidden_size=hidden, batch_first=True)\n",
    "        self.head = nn.Linear(hidden, n_assets)\n",
    "\n",
    "    def forward(self, r_hist):  # (B, window, N)\n",
    "        out, _ = self.lstm(r_hist)\n",
    "        h = out[:, -1, :]\n",
    "        pred = self.head(h)\n",
    "        p_hat = torch.exp(0.01 * pred)  # 양수 보장\n",
    "        return p_hat\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    \"\"\"\n",
    "    (과거 window 수익률 feature + prev_w + p_hat) -> 현재 포트폴리오 비중 w\n",
    "    cash(현금) 1개를 softmax에 같이 포함\n",
    "    \"\"\"\n",
    "    def __init__(self, n_assets: int, hidden_lstm: int = 64, hidden_mlp: int = 64, use_cash: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_cash = use_cash\n",
    "        self.lstm = nn.LSTM(input_size=n_assets, hidden_size=hidden_lstm, batch_first=True)\n",
    "\n",
    "        prev_dim = n_assets + (1 if use_cash else 0)\n",
    "        pred_dim = n_assets + (1 if use_cash else 0)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_lstm + prev_dim + pred_dim, hidden_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_mlp, prev_dim),  # logits\n",
    "        )\n",
    "\n",
    "    def forward(self, r_hist, prev_w, p_hat):\n",
    "        out, _ = self.lstm(r_hist)\n",
    "        h = out[:, -1, :]\n",
    "\n",
    "        if self.use_cash:\n",
    "            cash_pred = torch.ones((p_hat.size(0), 1), device=p_hat.device, dtype=p_hat.dtype)\n",
    "            p_hat = torch.cat([cash_pred, p_hat], dim=1)  # (B, 1+N)\n",
    "\n",
    "        x = torch.cat([h, prev_w, p_hat], dim=1)\n",
    "        logits = self.mlp(x)\n",
    "        w = F.softmax(logits, dim=1)\n",
    "        return w\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) \"논문 스타일\" 결정론 백테스트 + 학습 목적함수\n",
    "# =========================\n",
    "def turnover_cost(prev_w, w, fee_rate: float):\n",
    "    return fee_rate * torch.sum(torch.abs(w - prev_w), dim=1)  # (B,)\n",
    "\n",
    "\n",
    "def rollout_objective(\n",
    "    p_rel_seq,                 # (B, window+horizon, N)\n",
    "    window: int,\n",
    "    horizon: int,\n",
    "    predictor: nn.Module,\n",
    "    policy: nn.Module,\n",
    "    fee_rate: float = 0.0025,\n",
    "    use_cash: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    feature 추출(LSTM) + 포트폴리오 비중 산출 + 결정론 수익/비용 계산을\n",
    "    horizon 동안 굴려서 누적 log return 평균을 최대화(= objective 최대화)\n",
    "    \"\"\"\n",
    "    B, L, N = p_rel_seq.shape\n",
    "    device = p_rel_seq.device\n",
    "\n",
    "    r_seq = p_rel_seq - 1.0  # (B, L, N)\n",
    "\n",
    "    dim_w = N + (1 if use_cash else 0)\n",
    "    prev_w = torch.ones((B, dim_w), device=device) / dim_w\n",
    "\n",
    "    logrets = []\n",
    "\n",
    "    for t in range(horizon):\n",
    "        # (1) feature 추출 입력\n",
    "        r_hist = r_seq[:, t:t + window, :]              # (B, window, N)\n",
    "\n",
    "        # (2) predictor: p_hat\n",
    "        p_hat = predictor(r_hist)                        # (B, N)\n",
    "\n",
    "        # (3) policy: weights\n",
    "        w = policy(r_hist, prev_w, p_hat)                # (B, dim_w)\n",
    "\n",
    "        # (4) 다음 시점 실제 price relative로 수익 계산(결정론)\n",
    "        p_next = p_rel_seq[:, t + window, :]             # (B, N)\n",
    "        if use_cash:\n",
    "            cash = torch.ones((B, 1), device=device)\n",
    "            p_next = torch.cat([cash, p_next], dim=1)    # (B, 1+N)\n",
    "\n",
    "        # 거래비용\n",
    "        cost = turnover_cost(prev_w, w, fee_rate=fee_rate)\n",
    "        u = 1.0 - cost\n",
    "\n",
    "        gross = torch.sum(p_next * w, dim=1)             # (B,)\n",
    "        step_logret = torch.log(torch.clamp(u * gross, min=1e-12))\n",
    "        logrets.append(step_logret)\n",
    "\n",
    "        prev_w = w\n",
    "\n",
    "    logrets = torch.stack(logrets, dim=1)                # (B, horizon)\n",
    "    return torch.mean(torch.sum(logrets, dim=1))         # scalar\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) \"강화학습(학습루프)만\" 시간 측정 러너\n",
    "# =========================\n",
    "def run_rl_training_only_timebench(\n",
    "    prices_train: np.ndarray,\n",
    "    window: int = 31,\n",
    "    horizon: int = 32,\n",
    "    batch_size: int = 32,\n",
    "    iterations: int = 80_000,     # 논문 설정\n",
    "    bench_iters: int = 2000,      # 먼저 이만큼만 돌려서 it/s 측정\n",
    "    warmup_iters: int = 100,      # CUDA 워밍업\n",
    "    fee_rate: float = 0.0025,\n",
    "    lr: float = 3e-4,\n",
    "    seed: int = 42,\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "):\n",
    "    assert prices_train.ndim == 2 and np.isfinite(prices_train).all()\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if device.startswith(\"cuda\") and torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    p_train = price_relatives_from_prices(prices_train)  # (T-1, N)\n",
    "    Ttr, N = p_train.shape\n",
    "    assert Ttr > window + horizon + 2, \"train data too short: window/horizon 줄여\"\n",
    "\n",
    "    predictor = Predictor(n_assets=N, hidden=64).to(device)\n",
    "    policy    = PolicyNet(n_assets=N, hidden_lstm=64, hidden_mlp=64, use_cash=True).to(device)\n",
    "    optim = torch.optim.Adam(list(predictor.parameters()) + list(policy.parameters()), lr=lr)\n",
    "\n",
    "    # 배치 샘플링 (학습 파트에 포함되는 \"피처 입력\" 구성)\n",
    "    max_s = Ttr - (window + horizon) - 1\n",
    "    assert max_s > 0, \"train data too short for sampling\"\n",
    "    p_train_np = p_train  # numpy view\n",
    "\n",
    "    def sample_batch():\n",
    "        idx = np.random.randint(0, max_s, size=(batch_size,))\n",
    "        batch = np.stack([p_train_np[s:s + window + horizon, :] for s in idx], axis=0)  # (B, window+horizon, N)\n",
    "        x = torch.from_numpy(batch).to(device=device, dtype=torch.float32)\n",
    "        return x\n",
    "\n",
    "    def sync():\n",
    "        if device.startswith(\"cuda\") and torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    predictor.train(); policy.train()\n",
    "\n",
    "    print(\"\\n====================\")\n",
    "    print(\"RL TRAINING ONLY (feature(LSTM) + rollout + backward + step)\")\n",
    "    print(\"====================\")\n",
    "    print(f\"device={device} | assets={N}\")\n",
    "    print(f\"train_prices={prices_train.shape} | p_rel={p_train.shape}\")\n",
    "    print(f\"window={window} horizon={horizon} batch={batch_size} fee={fee_rate}\")\n",
    "    print(f\"warmup_iters={warmup_iters} | bench_iters={bench_iters} | target_iterations={iterations}\")\n",
    "\n",
    "    # ---- warmup ----\n",
    "    for _ in range(warmup_iters):\n",
    "        p_seq = sample_batch()\n",
    "        obj = rollout_objective(p_seq, window, horizon, predictor, policy, fee_rate=fee_rate, use_cash=True)\n",
    "        loss = -obj\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    sync()\n",
    "\n",
    "    # ---- measured bench (구간별 breakdown 포함) ----\n",
    "    t_sample = 0.0\n",
    "    t_forward = 0.0\n",
    "    t_backward = 0.0\n",
    "    t_step = 0.0\n",
    "\n",
    "    sync()\n",
    "    t0_all = time.perf_counter()\n",
    "\n",
    "    for it in range(bench_iters):\n",
    "        # 1) sample\n",
    "        t0 = time.perf_counter()\n",
    "        p_seq = sample_batch()\n",
    "        sync()\n",
    "        t_sample += time.perf_counter() - t0\n",
    "\n",
    "        # 2) forward+rollout (feature 추출 포함)\n",
    "        t0 = time.perf_counter()\n",
    "        obj = rollout_objective(p_seq, window, horizon, predictor, policy, fee_rate=fee_rate, use_cash=True)\n",
    "        loss = -obj\n",
    "        sync()\n",
    "        t_forward += time.perf_counter() - t0\n",
    "\n",
    "        # 3) backward\n",
    "        t0 = time.perf_counter()\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        sync()\n",
    "        t_backward += time.perf_counter() - t0\n",
    "\n",
    "        # 4) step\n",
    "        t0 = time.perf_counter()\n",
    "        optim.step()\n",
    "        sync()\n",
    "        t_step += time.perf_counter() - t0\n",
    "\n",
    "        if (it + 1) % 200 == 0:\n",
    "            elapsed = time.perf_counter() - t0_all\n",
    "            it_s = (it + 1) / max(elapsed, 1e-12)\n",
    "            eta_sec = iterations / max(it_s, 1e-12)\n",
    "            print(f\"[bench] {it+1}/{bench_iters}  speed={it_s:.3f} it/s  ETA(80k)≈{eta_sec/3600:.2f} h\")\n",
    "\n",
    "    sync()\n",
    "    t1_all = time.perf_counter()\n",
    "    total = t1_all - t0_all\n",
    "\n",
    "    it_s = bench_iters / max(total, 1e-12)\n",
    "    est_sec = iterations / max(it_s, 1e-12)\n",
    "\n",
    "    def per_iter_ms(x): return 1000.0 * x / max(bench_iters, 1)\n",
    "\n",
    "    print(\"\\n====================\")\n",
    "    print(\"RESULT (time only)\")\n",
    "    print(\"====================\")\n",
    "    print(f\"speed = {it_s:.3f} iterations/s\")\n",
    "    print(f\"EST: {iterations:,} iterations -> {est_sec/60:.1f} min ({est_sec/3600:.2f} h)\")\n",
    "    print(\n",
    "        \"per-iter avg(ms): \"\n",
    "        f\"sample={per_iter_ms(t_sample):.2f} | \"\n",
    "        f\"forward+rollout(feature포함)={per_iter_ms(t_forward):.2f} | \"\n",
    "        f\"backward={per_iter_ms(t_backward):.2f} | \"\n",
    "        f\"step={per_iter_ms(t_step):.2f} | \"\n",
    "        f\"total={per_iter_ms(total):.2f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) main: PPO 때랑 같은 데이터 크기로 생성 후 \"학습시간만\" 측정\n",
    "# =========================\n",
    "def main():\n",
    "    # (PPO 코드에서 쓰던 크기 그대로)\n",
    "    TRAIN_POINTS = 32220\n",
    "    N_ASSETS = 11\n",
    "    PPO_WINDOW = 30\n",
    "\n",
    "    PRICES_LEN_TRAIN = TRAIN_POINTS + PPO_WINDOW + 3\n",
    "    prices_train = make_synthetic_prices(T=PRICES_LEN_TRAIN, N=N_ASSETS, seed=42)\n",
    "\n",
    "    # 논문 기본: window=31, horizon=32, batch=32, iterations=80k\n",
    "    run_rl_training_only_timebench(\n",
    "        prices_train=prices_train,\n",
    "        window=31,\n",
    "        horizon=32,\n",
    "        batch_size=32,\n",
    "        iterations=80_000,\n",
    "        bench_iters=2000,\n",
    "        warmup_iters=100,\n",
    "        fee_rate=0.0025,\n",
    "        lr=3e-4,\n",
    "        seed=42,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a894ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
