from Data.process_data import process_data
from model_pickle.models import HMM_Model, PCA_Model, Cov_Model
from Env.trade_env import Environment
from Policy.opt_policy import ActorCritic
from Agent.ppo_agent import PPOAgent, Memory
import numpy as np
import torch
from torch_geometric.data import Data
import pandas as pd
import matplotlib.pyplot as plt
def observation_to_graph(observation, num_us_nodes, device='cpu'):
    """
    Args:
        observation: Env ë¦¬í„´ê°’
        num_us_nodes: ë¯¸êµ­ ì£¼ì‹ ê°œìˆ˜
    """
    regime = observation['regime']
    pca_vals = observation['pca']          # (4,)
    cov_matrix = observation['cov']        # (N_us+N_kr, N_us+N_kr)
    kor_feat = np.array(observation['kor_feat']) # (N_kr, 5)
    
    # [ìˆ˜ì •] Envì—ì„œ ë„˜ê²¨ì¤€ ë¯¸êµ­ ì£¼ì‹ íŠ¹ì§• ê°€ì ¸ì˜¤ê¸°
    # (N_us, 2) í˜•íƒœ -> [Price_Chg, Vol_Chg]
    us_feat = np.array(observation['us_feat'])   
    
    num_kr = len(kor_feat)
    num_pca = len(pca_vals) # 4
    num_regime = 3          # 3
    
    # -------------------------------------------------------
    # 1. Node Features êµ¬ì„± (Max Dim = 5)
    # -------------------------------------------------------
    
    # (1) [ìˆ˜ì •] ë¯¸êµ­ ì£¼ì‹ (N_us, 5): [Price_Chg, Vol_Chg, 0, 0, 0]
    # ì‹¤ì œ ë°ì´í„° 2ê°œ + 0 íŒ¨ë”© 3ê°œ = ì´ 5ê°œ
    us_tensor = torch.tensor(us_feat, dtype=torch.float, device=device)
    us_pad = torch.zeros((num_us_nodes, 3), device=device) # ë’¤ì— 3ê°œëŠ” 0ìœ¼ë¡œ ì±„ì›€
    x_us = torch.cat([us_tensor, us_pad], dim=1)
    
    # (2) í•œêµ­ ì£¼ì‹ (N_kr, 5): [Open, High, Low, Close, Volume]
    x_kr = torch.tensor(kor_feat, dtype=torch.float, device=device)
    
    # (3) PC ë…¸ë“œ (4, 5): [PCê°’, 0, 0, 0, 0]
    pca_tensor = torch.tensor(pca_vals, dtype=torch.float, device=device).view(-1, 1)
    pca_pad = torch.zeros((num_pca, 4), device=device)
    x_pca = torch.cat([pca_tensor, pca_pad], dim=1)
    
    # (4) Regime ë…¸ë“œ (3, 5): One-hot [1, 0, 0, 0, 0]
    regime_tensor = torch.zeros((num_regime, 5), device=device)
    regime_tensor[regime, 0] = 1.0 
    x_regime = regime_tensor
    
    # [ìµœì¢… Node Feature Matrix]
    # ìˆœì„œ: [US(Feat) | KR(Data) | PC | Regime]
    x = torch.cat([x_us, x_kr, x_pca, x_regime], dim=0)
    
    total_nodes = num_us_nodes + num_kr + num_pca + num_regime
    
    # -------------------------------------------------------
    # 2. Edge êµ¬ì„± (ë™ì¼)
    # -------------------------------------------------------
    edge_indices = []
    edge_attrs = []
    
    # (1) Stock-Stock (US & KR) Covariance Edge
    cov_tensor = torch.tensor(cov_matrix, dtype=torch.float, device=device)
    mask = torch.abs(cov_tensor) > 0.0
    stock_edge_idx = mask.nonzero().t()
    stock_edge_attr = cov_tensor[mask]
    
    edge_indices.append(stock_edge_idx)
    edge_attrs.append(stock_edge_attr)
    
    # (2) Stock <-> PC Edge (ëª¨ë“  ì£¼ì‹ê³¼ ì—°ê²°)
    stock_indices = torch.arange(num_us_nodes + num_kr, device=device)
    pca_indices = torch.arange(num_us_nodes + num_kr, num_us_nodes + num_kr + num_pca, device=device)
    
    s_grid, p_grid = torch.meshgrid(stock_indices, pca_indices, indexing='ij')
    sp_edges = torch.stack([s_grid.flatten(), p_grid.flatten()], dim=0)
    sp_attr = torch.ones(sp_edges.shape[1], device=device)
    
    edge_indices.append(sp_edges)
    edge_attrs.append(sp_attr)
    
    # (3) Stock <-> Regime Edge (ëª¨ë“  ì£¼ì‹ê³¼ ì—°ê²°)
    regime_indices = torch.arange(total_nodes - num_regime, total_nodes, device=device)
    
    s_grid, r_grid = torch.meshgrid(stock_indices, regime_indices, indexing='ij')
    sr_edges = torch.stack([s_grid.flatten(), r_grid.flatten()], dim=0)
    sr_attr = torch.ones(sr_edges.shape[1], device=device)
    
    edge_indices.append(sr_edges)
    edge_attrs.append(sr_attr)
    
    # -------------------------------------------------------
    # 3. ë°ì´í„° í•©ì¹˜ê¸°
    # -------------------------------------------------------
    # 1. ê¸°ë³¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    regime = observation['regime']
    pca_vals = observation['pca'] # (4,)
    cov_matrix = observation['cov'] # (N_stocks, N_stocks)
    
    # Envì—ì„œ í˜„ì¬ ë¹„ì¤‘(weights)ì„ ê°€ì ¸ì˜µë‹ˆë‹¤. (Cash í¬í•¨)
    # ì˜ˆ: [Cash, Stock1, Stock2, ...] í˜•íƒœë¼ê³  ê°€ì •
    current_weights = observation.get('weights', np.zeros(total_nodes - num_pca - num_regime + 1))
    
    # 2. í…ì„œ ë³€í™˜
    # (1) PCA (4ê°œ)
    pca_tensor_flat = torch.tensor(pca_vals, dtype=torch.float, device=device)
    
    # (2) Regime (3ê°œ, One-hot)
    regime_onehot = torch.zeros(3, device=device)
    regime_onehot[regime] = 1.0
    
    # (3) [Cov ë°˜ì˜] í˜„ì¬ í¬íŠ¸í´ë¦¬ì˜¤ ë¦¬ìŠ¤í¬ ê³„ì‚° (1ê°œ)
    # weightsì—ì„œ í˜„ê¸ˆ(ë³´í†µ ì²«ë²ˆì§¸ í˜¹ì€ ë§ˆì§€ë§‰)ì„ ì œì™¸í•˜ê³  ì£¼ì‹ ë¹„ì¤‘ë§Œ ê°€ì ¸ì™€ì•¼ í•¨
    # ì—¬ê¸°ì„œëŠ” weights[1:]ì´ ì£¼ì‹ ë¹„ì¤‘ì´ë¼ê³  ê°€ì • (Cashê°€ 0ë²ˆ ì¸ë±ìŠ¤ì¼ ê²½ìš°)
    # ë§Œì•½ weights ë§¨ ë’¤ê°€ í˜„ê¸ˆì´ë©´ weights[:-1] ì‚¬ìš©. 
    # **ì¤‘ìš”: cov_matrix í¬ê¸°ì™€ stock_weights í¬ê¸°ê°€ ê°™ì•„ì•¼ í•¨**
    
    stock_weights_np = current_weights[1:] # Cash ì œì™¸ (ê°€ì •)
    
    # í˜¹ì‹œ í¬ê¸°ê°€ ì•ˆ ë§ìœ¼ë©´ 0ìœ¼ë¡œ ì±„ìš°ê±°ë‚˜ ìë¦„ (ì•ˆì „ì¥ì¹˜)
    if len(stock_weights_np) != len(cov_matrix):
        # í¬ê¸° ë‹¤ë¥´ë©´ ë¦¬ìŠ¤í¬ 0ìœ¼ë¡œ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€)
        current_risk = 0.0
    else:
        # Risk = w^T * Cov * w
        # ê²°ê³¼ëŠ” ìŠ¤ì¹¼ë¼ ê°’ (ë‚´ í¬íŠ¸í´ë¦¬ì˜¤ì˜ ë¶„ì‚°)
        current_risk = np.dot(stock_weights_np.T, np.dot(cov_matrix, stock_weights_np))
        
    risk_tensor = torch.tensor([current_risk], dtype=torch.float, device=device)

    # 3. Gate ì…ë ¥ ë²¡í„° í•©ì¹˜ê¸° (Size: 4 + 3 + 1 = 8)
    gate_input = torch.cat([pca_tensor_flat, regime_onehot, risk_tensor], dim=0)

    # -------------------------------------------------------
    # Data ê°ì²´ì— í• ë‹¹
    # -------------------------------------------------------

    # ëª¨ë¸ë¡œ ì „ë‹¬í•  ë°ì´í„°


    final_edge_index = torch.cat(edge_indices, dim=1)
    final_edge_attr = torch.cat(edge_attrs, dim=0).view(-1, 1)
    
    # Mask ìƒì„±
    stock_mask = torch.zeros(total_nodes, dtype=torch.bool, device=device)
    stock_mask[num_us_nodes : num_us_nodes + num_kr] = True 
    
    data = Data(x=x, edge_index=final_edge_index, edge_attr=final_edge_attr)
    data.stock_mask = stock_mask
    data.old_weight = torch.tensor(current_weights, dtype=torch.float, device=device)
    data.input_data = gate_input.unsqueeze(0) # (1, 8) ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    return data
def train():
    # GPU ì„¤ì •
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Training Start on: {device}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("Loading Data...")
    loader = process_data()
    
    us_df = loader.fetch_us()
    kospi_df = loader.fetch_kospi()
    kor_df = loader.fetch_kor()
    bs_df = loader.fetch_bs()
    
    # ë°ì´í„° ì „ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° & ë‚ ì§œ ë³€í™˜)
    bs_df = bs_df.loc[:, ~bs_df.columns.duplicated()]
    us_df['Date'] = pd.to_datetime(us_df['Date'])
    kospi_df['Date'] = pd.to_datetime(kospi_df['Date'])
    kor_df['Date'] = pd.to_datetime(kor_df['Date'])
    if 'Date' in bs_df.columns:
        bs_df['Date'] = pd.to_datetime(bs_df['Date'])

    print("Data Loaded Successfully.")
    print(f"KR Stocks: {len(kor_df['Tick_id'].unique())} tickers")
    print(f"US Stocks: {len(us_df['Tick_id'].unique())} tickers")

    # 2. ëª¨ë¸ ë¡œë“œ
    hmm = HMM_Model('model_pickle/hmm_model.pkl')
    pca = PCA_Model('model_pickle/pca_model.pkl', 'model_pickle/scaler_model.pkl')
    cov = Cov_Model()
    
    # 3. í™˜ê²½ ì´ˆê¸°í™”
    env = Environment(
        time_window=20,
        budget=100_000_000,
        kor=kor_df,
        us=us_df,
        kfb=bs_df,
        kospi=kospi_df,
        pca_model=pca,
        hmm_model=hmm,
        cov=cov
    )
    
    # 4. Agent ì´ˆê¸°í™”
    model = ActorCritic(in_channels=5, hidden=64, heads=4, input_size=8).to(device)
    agent = PPOAgent(model, lr=0.0003, concentration=10.0, device=device)
    memory = Memory()
    
    # 5. í•˜ì´í¼íŒŒë¼ë¯¸í„°
    max_episodes = 100
    update_timestep = 200
    timestep = 0
    
    # [ğŸ”¥ ì¶”ê°€ 1] ì„±ëŠ¥ ê¸°ë¡ìš© ë¦¬ìŠ¤íŠ¸ ìƒì„±
    history = {'reward': [], 'portfolio_value': []}
    
    # =====================================================
    # í•™ìŠµ ì‹œì‘
    # =====================================================
    for episode in range(1, max_episodes + 1):
        state = env.reset()
        episode_reward = 0
        agent.concentration = 10.0 + (episode * 0.2)
        while True:
            timestep += 1
            
            num_us = env.us_ticker_size 
            graph_data = observation_to_graph(state, num_us, device=device)
            
            action, log_prob, value = agent.select_action(graph_data)
            next_state, reward, done, info = env.step(action)
            
            memory.states.append(graph_data.to('cpu')) 
            memory.actions.append(action)
            memory.log_probs.append(log_prob)
            memory.values.append(value)
            memory.rewards.append(reward)
            memory.is_terminals.append(done)
            
            state = next_state
            episode_reward += reward
            
            if timestep % update_timestep == 0:
                print(f" >> [Update] PPO Model Update at timestep {timestep}")
                agent.update(memory)
                memory.clear()
                
            if done:
                break
        
        # [ğŸ”¥ ì¶”ê°€ 2] ì´ë²ˆ ì—í”¼ì†Œë“œ ê²°ê³¼ ì €ì¥
        history['reward'].append(episode_reward)
        history['portfolio_value'].append(env.portfolio_value)
        
        print(f"Episode {episode}/{max_episodes} | Reward: {episode_reward:.2f} | PF Value: {env.portfolio_value:.0f}")
        
        if episode % 10 == 0:
            save_path = f"ppo_gat_ep{episode}.pth"
            torch.save(agent.model.state_dict(), save_path)
            print(f"Model saved to {save_path}")

    # =====================================================
    # [ğŸ”¥ ì¶”ê°€ 3] í•™ìŠµ ì¢…ë£Œ í›„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸° & ì €ì¥
    # =====================================================
    print("Training Finished. Saving results graph...")
    
    plt.figure(figsize=(12, 5))
    
    # 1. ë³´ìƒ(Reward) ê·¸ë˜í”„
    plt.subplot(1, 2, 1)
    plt.plot(history['reward'], label='Total Reward', color='blue')
    plt.title("Training Reward per Episode")
    plt.xlabel("Episode")
    plt.ylabel("Reward")
    plt.grid(True)
    
    # 2. í¬íŠ¸í´ë¦¬ì˜¤ ê°€ì¹˜(Portfolio Value) ê·¸ë˜í”„
    plt.subplot(1, 2, 2)
    plt.plot(history['portfolio_value'], label='Portfolio Value', color='orange')
    plt.axhline(y=100000000, color='red', linestyle='--', label='Initial Budget') # ì›ê¸ˆì„ 
    plt.title("Final Portfolio Value per Episode")
    plt.xlabel("Episode")
    plt.ylabel("Value (KRW)")
    plt.grid(True)
    plt.legend()
    
    # ê·¸ë˜í”„ íŒŒì¼ë¡œ ì €ì¥
    plt.savefig('training_result.png')
    print("Graph saved as 'training_result.png'. Check your folder!")
    plt.show() # ì°½ìœ¼ë¡œë„ ë„ìš°ê¸°

if __name__ == '__main__':
    train()