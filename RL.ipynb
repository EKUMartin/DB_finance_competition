{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b3eab6",
   "metadata": {},
   "source": [
    "# RL 구현 참고용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9c3882a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81ee6514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가짜 데이터 생성\n",
    "temp=[random.randint(-10,100) for _ in range(100)]\n",
    "property=[i*3.5+10+random.random()*2 for i in temp]\n",
    "data=pd.DataFrame({'x':temp,'y':property})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0169f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment 정의\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model=LinearRegression().fit(np.array(temp).reshape(-1,1),np.array(property).reshape((-1,1)))\n",
    "class Environment:\n",
    "    def __init__(self,z,setPoint):\n",
    "        self.z=z#현재 X값\n",
    "        self.setPoint=setPoint#target point\n",
    "        self.terminated=False#달성 여부\n",
    "        self.state=None\n",
    "    def reset(self):\n",
    "        yPred=model.predict(np.array([self.z]).reshape(-1,1)).item()\n",
    "        self.state=torch.tensor([self.setPoint-yPred])\n",
    "        return self.state\n",
    "    def step(self, action):\n",
    "        if action==0:\n",
    "            self.z-=1\n",
    "        elif action==1:\n",
    "            self.z+=1\n",
    "        if self.z<-10 or self.z>100:\n",
    "            reward=-100\n",
    "            self.terminated=True\n",
    "            return None, torch.tensor([reward]), self.terminated\n",
    "        self.state=torch.tensor([self.setPoint-model.predict(np.array([self.z]).reshape(-1,1)).item()])\n",
    "        if abs(self.state)<=5:\n",
    "            self.terminated=True\n",
    "        reward=1/abs(self.state)*5\n",
    "        return self.state, reward, self.terminated\n",
    "    def render(self):\n",
    "        return self.z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f11256d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RL 모델 정의\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.linear1=nn.Linear(1,16,bias=True)\n",
    "        self.linear2=nn.Linear(16,outputs,bias=True)\n",
    "    def forward(self,x):\n",
    "        x=x.to(device)\n",
    "        x=F.relu(self.linear1(x))\n",
    "        x=self.linear2(x)\n",
    "        return torch.unsqueeze(F.log_softmax(x,dim=0),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8fc1caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Action 정의\n",
    "BATCH_SIZE=128\n",
    "GAMMA=0.999\n",
    "EPS_START=0.9\n",
    "EPS_END=0.05\n",
    "EPS_DECAY=200\n",
    "TARGET_UPDATE=10\n",
    "n_actions=2\n",
    "policy_net=DQN(n_actions).to(device)\n",
    "target_net=DQN(n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "steps_done=0\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample=random.random()\n",
    "    eps_threshold=EPS_END+(EPS_START-EPS_END)*math.exp(-1*steps_done/EPS_DECAY)\n",
    "    steps_done+=1\n",
    "    if sample>eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).max(1)[1]\n",
    "    else:\n",
    "        return torch.tensor([random.randrange(n_actions)],device=device,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "beaa8464",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 정의\n",
    "Transition=namedtuple('Transition',('state','action','next_state','reward'))\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self,capacity):\n",
    "        self.memory=deque([],maxlen=capacity)\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "optimizer=optim.RMSprop(policy_net.parameters())\n",
    "memory=ReplayMemory(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bf30d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q learning\n",
    "def optimize_model():\n",
    "    if len(memory)<BATCH_SIZE:\n",
    "        return\n",
    "    transitions=memory.sample(BATCH_SIZE)\n",
    "    batch=Transition(*zip(*transitions))\n",
    "    non_final_mask=torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)),device=device, dtype=torch.bool)\n",
    "    non_final_next_states=torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch=torch.stack(batch.state)\n",
    "    action_batch=torch.cat(batch.action)\n",
    "    reward_batch=torch.cat(batch.reward)\n",
    "    state_action_values=policy_net(state_batch).squeeze().gather(1,action_batch.unsqueeze(1))\n",
    "    next_state_values=torch.zeros(BATCH_SIZE,device=device)\n",
    "    next_state_values[non_final_mask]=target_net(non_final_next_states.reshape(non_final_next_states.size()[0],1)).unsqueeze().max(1)[0].detach()\n",
    "    expected_state_action_values=(next_state_values*GAMMA)+reward_batch\n",
    "    criterion=nn.SmoothL1Loss()\n",
    "    loss=criterion(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1,1)\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f523294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setPoint 83 을 맞추기 위해\n",
      "0 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "1 -현재 X 41 에서 Up 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "2 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "3 -현재 X 39 에서 Down 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "4 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "5 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "6 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "7 -현재 X 37 에서 Down 하면 'setPoint-y'값은 tensor([-57.5204]) reward는 0.09\n",
      "8 -현재 X 36 에서 Down 하면 'setPoint-y'값은 tensor([-54.0216]) reward는 0.09\n",
      "9 -현재 X 37 에서 Up 하면 'setPoint-y'값은 tensor([-57.5204]) reward는 0.09\n",
      "10 -현재 X 38 에서 Up 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "11 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "12 -현재 X 40 에서 Up 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "13 -현재 X 39 에서 Down 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "14 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "15 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "16 -현재 X 40 에서 Up 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "17 -현재 X 41 에서 Up 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "18 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "19 -현재 X 41 에서 Up 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "20 -현재 X 42 에서 Up 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "21 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "22 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "23 -현재 X 41 에서 Down 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "24 -현재 X 42 에서 Up 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "25 -현재 X 41 에서 Down 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "26 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "27 -현재 X 41 에서 Up 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "28 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "29 -현재 X 41 에서 Up 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "30 -현재 X 42 에서 Up 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "31 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "32 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "33 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "34 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "35 -현재 X 41 에서 Down 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "36 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "37 -현재 X 39 에서 Down 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "38 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "39 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "40 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "41 -현재 X 37 에서 Down 하면 'setPoint-y'값은 tensor([-57.5204]) reward는 0.09\n",
      "42 -현재 X 38 에서 Up 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "43 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "44 -현재 X 40 에서 Up 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "45 -현재 X 41 에서 Up 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "46 -현재 X 42 에서 Up 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "47 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "48 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "49 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "50 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "51 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "52 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "53 -현재 X 41 에서 Down 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "54 -현재 X 42 에서 Up 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "55 -현재 X 43 에서 Up 하면 'setPoint-y'값은 tensor([-78.5129]) reward는 0.06\n",
      "56 -현재 X 42 에서 Down 하면 'setPoint-y'값은 tensor([-75.0142]) reward는 0.07\n",
      "57 -현재 X 41 에서 Down 하면 'setPoint-y'값은 tensor([-71.5154]) reward는 0.07\n",
      "58 -현재 X 40 에서 Down 하면 'setPoint-y'값은 tensor([-68.0167]) reward는 0.07\n",
      "59 -현재 X 39 에서 Down 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "60 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "61 -현재 X 37 에서 Down 하면 'setPoint-y'값은 tensor([-57.5204]) reward는 0.09\n",
      "62 -현재 X 38 에서 Up 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "63 -현재 X 37 에서 Down 하면 'setPoint-y'값은 tensor([-57.5204]) reward는 0.09\n",
      "64 -현재 X 38 에서 Up 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "65 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "66 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "67 -현재 X 39 에서 Up 하면 'setPoint-y'값은 tensor([-64.5179]) reward는 0.08\n",
      "68 -현재 X 38 에서 Down 하면 'setPoint-y'값은 tensor([-61.0191]) reward는 0.08\n",
      "69 -현재 X 37 에서 Down 하면 'setPoint-y'값은 tensor([-57.5204]) reward는 0.09\n",
      "70 -현재 X 36 에서 Down 하면 'setPoint-y'값은 tensor([-54.0216]) reward는 0.09\n",
      "71 -현재 X 35 에서 Down 하면 'setPoint-y'값은 tensor([-50.5229]) reward는 0.1\n",
      "72 -현재 X 34 에서 Down 하면 'setPoint-y'값은 tensor([-47.0241]) reward는 0.11\n",
      "73 -현재 X 33 에서 Down 하면 'setPoint-y'값은 tensor([-43.5254]) reward는 0.11\n",
      "74 -현재 X 32 에서 Down 하면 'setPoint-y'값은 tensor([-40.0266]) reward는 0.12\n",
      "75 -현재 X 31 에서 Down 하면 'setPoint-y'값은 tensor([-36.5278]) reward는 0.14\n",
      "76 -현재 X 30 에서 Down 하면 'setPoint-y'값은 tensor([-33.0291]) reward는 0.15\n",
      "77 -현재 X 29 에서 Down 하면 'setPoint-y'값은 tensor([-29.5303]) reward는 0.17\n",
      "78 -현재 X 28 에서 Down 하면 'setPoint-y'값은 tensor([-26.0316]) reward는 0.19\n",
      "79 -현재 X 27 에서 Down 하면 'setPoint-y'값은 tensor([-22.5328]) reward는 0.22\n",
      "80 -현재 X 28 에서 Up 하면 'setPoint-y'값은 tensor([-26.0316]) reward는 0.19\n",
      "81 -현재 X 27 에서 Down 하면 'setPoint-y'값은 tensor([-22.5328]) reward는 0.22\n",
      "82 -현재 X 28 에서 Up 하면 'setPoint-y'값은 tensor([-26.0316]) reward는 0.19\n",
      "83 -현재 X 27 에서 Down 하면 'setPoint-y'값은 tensor([-22.5328]) reward는 0.22\n",
      "84 -현재 X 26 에서 Down 하면 'setPoint-y'값은 tensor([-19.0341]) reward는 0.26\n",
      "85 -현재 X 27 에서 Up 하면 'setPoint-y'값은 tensor([-22.5328]) reward는 0.22\n",
      "86 -현재 X 26 에서 Down 하면 'setPoint-y'값은 tensor([-19.0341]) reward는 0.26\n",
      "87 -현재 X 27 에서 Up 하면 'setPoint-y'값은 tensor([-22.5328]) reward는 0.22\n",
      "88 -현재 X 26 에서 Down 하면 'setPoint-y'값은 tensor([-19.0341]) reward는 0.26\n",
      "89 -현재 X 25 에서 Down 하면 'setPoint-y'값은 tensor([-15.5353]) reward는 0.32\n",
      "90 -현재 X 24 에서 Down 하면 'setPoint-y'값은 tensor([-12.0365]) reward는 0.42\n",
      "91 -현재 X 25 에서 Up 하면 'setPoint-y'값은 tensor([-15.5353]) reward는 0.32\n",
      "92 -현재 X 24 에서 Down 하면 'setPoint-y'값은 tensor([-12.0365]) reward는 0.42\n",
      "93 -현재 X 23 에서 Down 하면 'setPoint-y'값은 tensor([-8.5378]) reward는 0.59\n",
      "94 -현재 X 24 에서 Up 하면 'setPoint-y'값은 tensor([-12.0365]) reward는 0.42\n",
      "95 -현재 X 25 에서 Up 하면 'setPoint-y'값은 tensor([-15.5353]) reward는 0.32\n",
      "96 -현재 X 24 에서 Down 하면 'setPoint-y'값은 tensor([-12.0365]) reward는 0.42\n",
      "97 -현재 X 23 에서 Down 하면 'setPoint-y'값은 tensor([-8.5378]) reward는 0.59\n",
      "98 -현재 X 24 에서 Up 하면 'setPoint-y'값은 tensor([-12.0365]) reward는 0.42\n",
      "99 -현재 X 23 에서 Down 하면 'setPoint-y'값은 tensor([-8.5378]) reward는 0.59\n",
      "100 -현재 X 24 에서 Up 하면 'setPoint-y'값은 tensor([-12.0365]) reward는 0.42\n",
      "101 -현재 X 23 에서 Down 하면 'setPoint-y'값은 tensor([-8.5378]) reward는 0.59\n",
      "102 -현재 X 22 에서 Down 하면 'setPoint-y'값은 tensor([-5.0390]) reward는 0.99\n",
      "103 -현재 X 21 에서 Down 하면 'setPoint-y'값은 tensor([-1.5403]) reward는 3.25\n",
      "성공!\n",
      "\n",
      "complete\n",
      "setPoint 260 을 맞추기 위해\n",
      "0 -현재 X 66 에서 Up 하면 'setPoint-y'값은 tensor([18.0156]) reward는 0.28\n",
      "1 -현재 X 65 에서 Down 하면 'setPoint-y'값은 tensor([21.5144]) reward는 0.23\n",
      "2 -현재 X 66 에서 Up 하면 'setPoint-y'값은 tensor([18.0156]) reward는 0.28\n",
      "3 -현재 X 65 에서 Down 하면 'setPoint-y'값은 tensor([21.5144]) reward는 0.23\n",
      "4 -현재 X 66 에서 Up 하면 'setPoint-y'값은 tensor([18.0156]) reward는 0.28\n",
      "5 -현재 X 65 에서 Down 하면 'setPoint-y'값은 tensor([21.5144]) reward는 0.23\n",
      "6 -현재 X 66 에서 Up 하면 'setPoint-y'값은 tensor([18.0156]) reward는 0.28\n",
      "7 -현재 X 65 에서 Down 하면 'setPoint-y'값은 tensor([21.5144]) reward는 0.23\n",
      "8 -현재 X 64 에서 Down 하면 'setPoint-y'값은 tensor([25.0132]) reward는 0.2\n",
      "9 -현재 X 63 에서 Down 하면 'setPoint-y'값은 tensor([28.5119]) reward는 0.18\n",
      "10 -현재 X 64 에서 Up 하면 'setPoint-y'값은 tensor([25.0132]) reward는 0.2\n",
      "11 -현재 X 65 에서 Up 하면 'setPoint-y'값은 tensor([21.5144]) reward는 0.23\n",
      "12 -현재 X 66 에서 Up 하면 'setPoint-y'값은 tensor([18.0156]) reward는 0.28\n",
      "13 -현재 X 67 에서 Up 하면 'setPoint-y'값은 tensor([14.5169]) reward는 0.34\n",
      "14 -현재 X 68 에서 Up 하면 'setPoint-y'값은 tensor([11.0181]) reward는 0.45\n",
      "15 -현재 X 69 에서 Up 하면 'setPoint-y'값은 tensor([7.5194]) reward는 0.66\n",
      "16 -현재 X 70 에서 Up 하면 'setPoint-y'값은 tensor([4.0206]) reward는 1.24\n",
      "성공!\n",
      "\n",
      "complete\n",
      "setPoint 163 을 맞추기 위해\n",
      "0 -현재 X 12 에서 Up 하면 'setPoint-y'값은 tensor([109.9485]) reward는 0.05\n",
      "1 -현재 X 13 에서 Up 하면 'setPoint-y'값은 tensor([106.4498]) reward는 0.05\n",
      "2 -현재 X 12 에서 Down 하면 'setPoint-y'값은 tensor([109.9485]) reward는 0.05\n",
      "3 -현재 X 11 에서 Down 하면 'setPoint-y'값은 tensor([113.4473]) reward는 0.04\n",
      "4 -현재 X 10 에서 Down 하면 'setPoint-y'값은 tensor([116.9461]) reward는 0.04\n",
      "5 -현재 X 9 에서 Down 하면 'setPoint-y'값은 tensor([120.4448]) reward는 0.04\n",
      "6 -현재 X 8 에서 Down 하면 'setPoint-y'값은 tensor([123.9436]) reward는 0.04\n",
      "7 -현재 X 7 에서 Down 하면 'setPoint-y'값은 tensor([127.4423]) reward는 0.04\n",
      "8 -현재 X 6 에서 Down 하면 'setPoint-y'값은 tensor([130.9411]) reward는 0.04\n",
      "9 -현재 X 7 에서 Up 하면 'setPoint-y'값은 tensor([127.4423]) reward는 0.04\n",
      "10 -현재 X 6 에서 Down 하면 'setPoint-y'값은 tensor([130.9411]) reward는 0.04\n",
      "11 -현재 X 5 에서 Down 하면 'setPoint-y'값은 tensor([134.4398]) reward는 0.04\n",
      "12 -현재 X 4 에서 Down 하면 'setPoint-y'값은 tensor([137.9386]) reward는 0.04\n",
      "13 -현재 X 3 에서 Down 하면 'setPoint-y'값은 tensor([141.4374]) reward는 0.04\n",
      "14 -현재 X 2 에서 Down 하면 'setPoint-y'값은 tensor([144.9361]) reward는 0.03\n",
      "15 -현재 X 1 에서 Down 하면 'setPoint-y'값은 tensor([148.4349]) reward는 0.03\n",
      "16 -현재 X 0 에서 Down 하면 'setPoint-y'값은 tensor([151.9336]) reward는 0.03\n",
      "17 -현재 X -1 에서 Down 하면 'setPoint-y'값은 tensor([155.4324]) reward는 0.03\n",
      "18 -현재 X -2 에서 Down 하면 'setPoint-y'값은 tensor([158.9311]) reward는 0.03\n",
      "19 -현재 X -3 에서 Down 하면 'setPoint-y'값은 tensor([162.4299]) reward는 0.03\n",
      "20 -현재 X -2 에서 Up 하면 'setPoint-y'값은 tensor([158.9311]) reward는 0.03\n",
      "21 -현재 X -3 에서 Down 하면 'setPoint-y'값은 tensor([162.4299]) reward는 0.03\n",
      "22 -현재 X -4 에서 Down 하면 'setPoint-y'값은 tensor([165.9287]) reward는 0.03\n",
      "23 -현재 X -3 에서 Up 하면 'setPoint-y'값은 tensor([162.4299]) reward는 0.03\n",
      "24 -현재 X -4 에서 Down 하면 'setPoint-y'값은 tensor([165.9287]) reward는 0.03\n",
      "25 -현재 X -3 에서 Up 하면 'setPoint-y'값은 tensor([162.4299]) reward는 0.03\n",
      "26 -현재 X -4 에서 Down 하면 'setPoint-y'값은 tensor([165.9287]) reward는 0.03\n",
      "27 -현재 X -3 에서 Up 하면 'setPoint-y'값은 tensor([162.4299]) reward는 0.03\n",
      "28 -현재 X -4 에서 Down 하면 'setPoint-y'값은 tensor([165.9287]) reward는 0.03\n",
      "29 -현재 X -5 에서 Down 하면 'setPoint-y'값은 tensor([169.4274]) reward는 0.03\n",
      "30 -현재 X -6 에서 Down 하면 'setPoint-y'값은 tensor([172.9262]) reward는 0.03\n",
      "31 -현재 X -7 에서 Down 하면 'setPoint-y'값은 tensor([176.4249]) reward는 0.03\n",
      "32 -현재 X -8 에서 Down 하면 'setPoint-y'값은 tensor([179.9237]) reward는 0.03\n",
      "33 -현재 X -7 에서 Up 하면 'setPoint-y'값은 tensor([176.4249]) reward는 0.03\n",
      "34 -현재 X -8 에서 Down 하면 'setPoint-y'값은 tensor([179.9237]) reward는 0.03\n",
      "35 -현재 X -9 에서 Down 하면 'setPoint-y'값은 tensor([183.4224]) reward는 0.03\n",
      "36 -현재 X -10 에서 Down 하면 'setPoint-y'값은 tensor([186.9212]) reward는 0.03\n",
      "37 -현재 X -11 에서 Down 하면 'setPoint-y'값은 None reward는 -100\n",
      "실패\n",
      "\n",
      "complete\n",
      "setPoint 52 을 맞추기 위해\n",
      "0 -현재 X 20 에서 Down 하면 'setPoint-y'값은 tensor([-29.0415]) reward는 0.17\n",
      "1 -현재 X 19 에서 Down 하면 'setPoint-y'값은 tensor([-25.5428]) reward는 0.2\n",
      "2 -현재 X 20 에서 Up 하면 'setPoint-y'값은 tensor([-29.0415]) reward는 0.17\n",
      "3 -현재 X 21 에서 Up 하면 'setPoint-y'값은 tensor([-32.5403]) reward는 0.15\n",
      "4 -현재 X 20 에서 Down 하면 'setPoint-y'값은 tensor([-29.0415]) reward는 0.17\n",
      "5 -현재 X 19 에서 Down 하면 'setPoint-y'값은 tensor([-25.5428]) reward는 0.2\n",
      "6 -현재 X 18 에서 Down 하면 'setPoint-y'값은 tensor([-22.0440]) reward는 0.23\n",
      "7 -현재 X 17 에서 Down 하면 'setPoint-y'값은 tensor([-18.5452]) reward는 0.27\n",
      "8 -현재 X 16 에서 Down 하면 'setPoint-y'값은 tensor([-15.0465]) reward는 0.33\n",
      "9 -현재 X 15 에서 Down 하면 'setPoint-y'값은 tensor([-11.5477]) reward는 0.43\n",
      "10 -현재 X 16 에서 Up 하면 'setPoint-y'값은 tensor([-15.0465]) reward는 0.33\n",
      "11 -현재 X 17 에서 Up 하면 'setPoint-y'값은 tensor([-18.5452]) reward는 0.27\n",
      "12 -현재 X 16 에서 Down 하면 'setPoint-y'값은 tensor([-15.0465]) reward는 0.33\n",
      "13 -현재 X 15 에서 Down 하면 'setPoint-y'값은 tensor([-11.5477]) reward는 0.43\n",
      "14 -현재 X 14 에서 Down 하면 'setPoint-y'값은 tensor([-8.0490]) reward는 0.62\n",
      "15 -현재 X 13 에서 Down 하면 'setPoint-y'값은 tensor([-4.5502]) reward는 1.1\n",
      "성공!\n",
      "\n",
      "complete\n",
      "setPoint 214 을 맞추기 위해\n",
      "0 -현재 X -5 에서 Down 하면 'setPoint-y'값은 tensor([220.4274]) reward는 0.02\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsqueeze() missing 1 required positional arguments: \"dim\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[94], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m epMemory\u001b[38;5;241m.\u001b[39mappend([state,action,next_state,reward])\n\u001b[0;32m     21\u001b[0m state\u001b[38;5;241m=\u001b[39mnext_state\n\u001b[1;32m---> 22\u001b[0m \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender()\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m:\n",
      "Cell \u001b[1;32mIn[93], line 14\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m state_action_values\u001b[38;5;241m=\u001b[39mpolicy_net(state_batch)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mgather(\u001b[38;5;241m1\u001b[39m,action_batch\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     13\u001b[0m next_state_values\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mzeros(BATCH_SIZE,device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m---> 14\u001b[0m next_state_values[non_final_mask]\u001b[38;5;241m=\u001b[39m\u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m     15\u001b[0m expected_state_action_values\u001b[38;5;241m=\u001b[39m(next_state_values\u001b[38;5;241m*\u001b[39mGAMMA)\u001b[38;5;241m+\u001b[39mreward_batch\n\u001b[0;32m     16\u001b[0m criterion\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mSmoothL1Loss()\n",
      "\u001b[1;31mTypeError\u001b[0m: unsqueeze() missing 1 required positional arguments: \"dim\""
     ]
    }
   ],
   "source": [
    "def Action(x):\n",
    "    if x.item()==0:\n",
    "        return \"Down\"\n",
    "    else:\n",
    "        return \"Up\"\n",
    "    \n",
    "num_episodes=100\n",
    "for i_episode in range(num_episodes):\n",
    "    epMemory=list()\n",
    "    z=random.randrange(-10,100)\n",
    "    setPoint=random.randrange(math.floor(min(data['y'])), math.ceil(max(data['y'])))\n",
    "    env=Environment(z=z,setPoint=setPoint)\n",
    "    state=env.reset()\n",
    "    print(\"setPoint\", setPoint, \"을 맞추기 위해\")\n",
    "    for t in count():\n",
    "        action=select_action(torch.tensor([state]).float())\n",
    "        next_state, reward, done=env.step(action.item())\n",
    "        reward=torch.tensor([reward],device=device)\n",
    "        print(t,\"-현재 X\",env.render(),\"에서\",Action(action),\"하면 'setPoint-y'값은\",next_state,\"reward는\",round(reward.item(),2))\n",
    "        epMemory.append([state,action,next_state,reward])\n",
    "        state=next_state\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            if env.render()>=-10 and env.render()<=100:\n",
    "                _=[memory.push(epMemory[i][0],epMemory[i][1],epMemory[i][2],epMemory[i][3]) for i in range(len(epMemory))]\n",
    "                print(\"성공!\")\n",
    "            else:\n",
    "                print(\"실패\")\n",
    "            print(\"\")\n",
    "            break\n",
    "        if t>=100000:\n",
    "            print(\"중단!\")\n",
    "            break\n",
    "    if i_episode % TARGET_UPDATE==0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    print('complete')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20160d1",
   "metadata": {},
   "source": [
    "# 포트폴리오 최적화 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6e797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#라이브러리\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pandas as pd\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb9ae2",
   "metadata": {},
   "source": [
    "## 필요 함수, 객체들 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8adc903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#미국 주식 데이터 데이터 \n",
    "import yfinance as yf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "tickers = [\"XLK\",\"XLF\",\"XLV\",\"XLY\",\"XLP\",\"XLC\",\"XLI\",\"XLE\",\"XLB\",\"XLU\",\"XLRE\"]\n",
    "\n",
    "df_close = yf.download(\n",
    "    tickers,\n",
    "    start=\"2015-01-01\",\n",
    "    interval=\"1d\",\n",
    "    auto_adjust=True,\n",
    "    progress=False\n",
    ")[\"Close\"]\n",
    "split_idx = int(len(df_close) * 0.8)\n",
    "train_close = df_close.iloc[:split_idx]\n",
    "test_close  = df_close.iloc[split_idx:]\n",
    "class TickerSeqDataset(Dataset):\n",
    "    def __init__(self, close_df: pd.DataFrame, window: int, use_zscore: bool = True):\n",
    "        self.close_df = close_df\n",
    "        self.window = window\n",
    "        self.use_zscore = use_zscore\n",
    "\n",
    "        self.tickers = list(close_df.columns)\n",
    "        self.series = {}\n",
    "        self.mu = {}\n",
    "        self.sd = {}\n",
    "        for t in self.tickers:\n",
    "            s = close_df[t].dropna().values.astype(np.float32)\n",
    "            self.series[t] = s\n",
    "\n",
    "            if use_zscore:\n",
    "                m = float(s.mean())\n",
    "                v = float(s.std())\n",
    "                self.mu[t] = m\n",
    "                self.sd[t] = v if v > 1e-8 else 1.0\n",
    "        self.index = []\n",
    "        for ti, t in enumerate(self.tickers):\n",
    "            L = len(self.series[t])\n",
    "            if L >= window + 1:\n",
    "                for end in range(window - 1, L - 1):\n",
    "                    self.index.append((ti, end))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ti, end = self.index[idx]\n",
    "        t = self.tickers[ti]\n",
    "        s = self.series[t]\n",
    "\n",
    "        x = s[end - self.window + 1 : end + 1]  \n",
    "        y = s[end + 1]                            \n",
    "        if self.use_zscore:\n",
    "            x = (x - self.mu[t]) / self.sd[t]\n",
    "            y = (y - self.mu[t]) / self.sd[t]\n",
    "\n",
    "        x_seq = torch.from_numpy(x).unsqueeze(-1)  \n",
    "        y_t = torch.tensor([y], dtype=torch.float32)      \n",
    "        return x_seq, y_t\n",
    "\n",
    "window_size = 30\n",
    "batch_size = 64\n",
    "\n",
    "train_ds = TickerSeqDataset(train_close, window=window_size, use_zscore=True)\n",
    "test_ds  = TickerSeqDataset(test_close,  window=window_size, use_zscore=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "# 4) 모델 차원 (종목 단위이므로 1->1)\n",
    "input_dim = 1\n",
    "output_dim = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764235aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국 데이터 준비\n",
    "import FinanceDataReader as fdr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df934f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.055490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[126], line 10\u001b[0m\n",
      "\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n",
      "\u001b[0;32m      3\u001b[0m us_lstm \u001b[38;5;241m=\u001b[39m us_bol(\n",
      "\u001b[0;32m      4\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n",
      "\u001b[0;32m      5\u001b[0m         hidden_dim\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m      8\u001b[0m         window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n",
      "\u001b[0;32m      9\u001b[0m     )\n",
      "\u001b[1;32m---> 10\u001b[0m us_bol_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mus_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     11\u001b[0m us_lstm_loss\u001b[38;5;241m=\u001b[39mevaluate_model(us_bol_trained,test_loader)\n",
      "\n",
      "Cell \u001b[1;32mIn[124], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, config)\u001b[0m\n",
      "\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_seq)\n",
      "\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n",
      "\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n",
      "\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n",
      "\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n",
      "\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n",
      "\u001b[0;32m    580\u001b[0m     )\n",
      "\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n",
      "\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n",
      "\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n",
      "\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n",
      "\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n",
      "\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n",
      "\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n",
      "\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n",
      "\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n",
      "\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델들 사전 학습\n",
    "from sklearn.linear_model import LinearRegression\n",
    "us_lstm = us_bol(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        out_dim=output_dim,\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        window_size=20\n",
    "    )\n",
    "us_bol_trained = train_model(us_lstm, train_loader, CONFIG)\n",
    "us_lstm_loss=evaluate_model(us_bol_trained,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4706848a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 커스텀 손실함수\n",
    "class loss_fucntion():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def loss_mse(y,y_pred):\n",
    "        return ((y-y_pred)**2).mean()\n",
    "\n",
    "#미국 주식 볼린저밴드 계산(LSTM)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class us_bol(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, out_dim, window_size):  # out_dim = 종목 수\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_dim, out_dim)\n",
    "        self.window = window_size\n",
    "    def forward(self, x_seq):\n",
    "        out, (h_n, c_n) = self.lstm(x_seq)\n",
    "        y_pred = self.head(h_n[-1])      \n",
    "        return y_pred\n",
    "\n",
    "def bollinger_pred_break_nextband(self, y_pred_next, df_close_upto_prev):\n",
    "    y = y_pred_next[0] if y_pred_next.dim() == 2 else y_pred_next\n",
    "    y_np = y.detach().cpu().numpy()\n",
    "    df_ext = df_close_upto_prev.copy()\n",
    "    y_np =df_ext.loc[df_ext.index[-1] + pd.Timedelta(days=1)]\n",
    "    ma = df_ext.rolling(self.window).mean()\n",
    "    std = df_ext.rolling(self.window).std()\n",
    "    upper_next = ma.iloc[-1].to_numpy()\n",
    "    lower_next = ma.iloc[-1].to_numpy()\n",
    "    upper_next = ma.iloc[-1].to_numpy() + 2 * std.iloc[-1].to_numpy()\n",
    "    lower_next = ma.iloc[-1].to_numpy() - 2 * std.iloc[-1].to_numpy()\n",
    "    flags = np.zeros_like(y_np, dtype=np.int64)\n",
    "    flags[y_np > upper_next] = 1\n",
    "    flags[y_np < lower_next] = -1\n",
    "    return torch.tensor(flags, device=y_pred_next.device)\n",
    "\n",
    " \n",
    "#한국 주식 correlation, 지표, 거래량 같은 거 계산\n",
    "class kor():\n",
    "    def __init__(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f070026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#피처 추출 모델 학습\n",
    "def train_model(model, train_loader, config):\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "    \n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for X_seq,y in train_loader:\n",
    "            X_seq = X_seq.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_seq)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * X_seq.size(0)\n",
    "        avg_loss = total_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{config['epochs']}], Loss: {avg_loss:.6f}\")\n",
    "    return model\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    criterion=nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for x_seq,y in test_loader:\n",
    "            x_seq = x_seq.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(x_seq)    \n",
    "            loss = criterion(pred, y)\n",
    "            total_loss += loss.item()\n",
    "            total_n += y.size(0)\n",
    "\n",
    "    \n",
    "    return total_loss / total_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d45f401e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG={\n",
    "    \"hidden_dim\":64,\n",
    "    \"num_layers\":12,\n",
    "    \"learning_rate\":0.1,\n",
    "    \"epochs\":50\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1ebc0209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 1.055490\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m us_lstm \u001b[38;5;241m=\u001b[39m us_bol(\n\u001b[0;32m      4\u001b[0m         input_dim\u001b[38;5;241m=\u001b[39minput_dim,\n\u001b[0;32m      5\u001b[0m         hidden_dim\u001b[38;5;241m=\u001b[39mCONFIG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      8\u001b[0m         window_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      9\u001b[0m     )\n\u001b[1;32m---> 10\u001b[0m us_bol_trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mus_lstm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m us_lstm_loss\u001b[38;5;241m=\u001b[39mevaluate_model(us_bol_trained,test_loader)\n",
      "Cell \u001b[1;32mIn[124], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, config)\u001b[0m\n\u001b[0;32m     14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_seq)\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     18\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m X_seq\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 모델들 사전 학습\n",
    "from sklearn.linear_model import LinearRegression\n",
    "us_lstm = us_bol(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=CONFIG[\"hidden_dim\"],\n",
    "        out_dim=output_dim,\n",
    "        num_layers=CONFIG[\"num_layers\"],\n",
    "        window_size=20\n",
    "    )\n",
    "us_bol_trained = train_model(us_lstm, train_loader, CONFIG)\n",
    "us_lstm_loss=evaluate_model(us_bol_trained,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272726f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# environment 정의\n",
    "class Environment:\n",
    "    def __init__(self, df_close_us: pd.DataFrame, df_close_kr:pd.DataFrame, predictor, device,\n",
    "                 seq_window=30, bol_window=20, cost_rate=0.0005,\n",
    "                 normalize_action=True):\n",
    "        self.df_us = df_close_us.dropna().copy()\n",
    "        self.df_kr = df_close_kr.dropna().copy()\n",
    "        self.predictor = predictor.to(device)\n",
    "        self.device = device\n",
    "        self.seq_window = seq_window\n",
    "        self.bol_window = bol_window\n",
    "        self.cost_rate = cost_rate\n",
    "        self.normalize_action = normalize_action\n",
    "        self.tickers = list(self.df_kr.columns)\n",
    "        self.N = len(self.tickers)\n",
    "        self.t = None\n",
    "        self.prev_w = None\n",
    "\n",
    "    def reset(self,start_idx=None):\n",
    "        self.t = (self.seq_window - 1 if start_idx is None else max(start_idx, self.seq_window - 1))\n",
    "        self.prev_w = np.ones(self.N, dtype=np.float32) / self.N\n",
    "        return self._get_state()\n",
    "\n",
    "    def step(self, action):\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            w = action.detach().cpu().numpy().astype(np.float32).reshape(-1)\n",
    "        else:\n",
    "            w = np.asarray(action, dtype=np.float32).reshape(-1)\n",
    "        #softmax로 변환\n",
    "        expw = np.exp(w - np.max(w))\n",
    "        w = expw / (np.sum(expw) + 1e-12)\n",
    "        turnover = float(np.sum(np.abs(w - self.prev_w)))\n",
    "        cost = self.cost_rate * turnover\n",
    "        t0 = self.t\n",
    "        t1 = self.t + 1\n",
    "        if t1 >= len(self.df_kr):\n",
    "            return None, 0.0, True, {\"reason\": \"end_of_data\"}\n",
    "        p0 = self.df_kr.iloc[t0].values.astype(np.float32)  \n",
    "        p1 = self.df_kr.iloc[t1].values.astype(np.float32) \n",
    "        asset_ret = (p1 / (p0 + 1e-12)) - 1.0            \n",
    "        port_ret = float(np.dot(w, asset_ret))\n",
    "        reward = port_ret - cost\n",
    "        self.prev_w = w\n",
    "        self.t = t1\n",
    "        next_state = self._get_state()\n",
    "        info = {\"t\": self.t, \"port_ret\": port_ret, \"cost\": cost, \"turnover\": turnover}\n",
    "        done = False\n",
    "        return next_state, reward, done, info\n",
    "    def _get_state(self):\n",
    "        t = self.t\n",
    "        us_aligned = self.df_us.reindex(self.df_kr.index, method=\"ffill\").shift(1)\n",
    "        df_upto_t_us = us_aligned.iloc[:t+1]\n",
    "        df_upto_t_kr = self.df_kr.iloc[:t+1]\n",
    "        cur_prices = df_upto_t_kr.iloc[-1].values.astype(np.float32)\n",
    "        pred_break = self.predictor.get_pred_break(df_upto_t_us, t)\n",
    "        prev_w = self.prev_w.astype(np.float32)\n",
    "        state = torch.tensor(\n",
    "            np.concatenate([cur_prices, pred_break, prev_w], axis=0),\n",
    "            dtype=torch.float32,\n",
    "            device=self.device\n",
    "        )\n",
    "        return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcbedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom policy에서 사용할 네트워크 만들기\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self, features_dim: int, latent_dim_pi: int = 64, latent_dim_vf: int = 64):\n",
    "        super().__init__()\n",
    "        self.latent_dim_pi = latent_dim_pi\n",
    "        self.latent_dim_vf = latent_dim_vf\n",
    "\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(features_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim_pi),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(features_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim_vf),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, features: torch.Tensor):\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: torch.Tensor):\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: torch.Tensor):\n",
    "        return self.value_net(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34a6685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom policy 만들기(PPO)기반\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "class CustomPolicy(ActorCriticPolicy):\n",
    "    def __init__(self, observation_space, action_space, lr_schedule,\n",
    "                 net_arch=None, activation_fn=nn.Tanh, ortho_init=False, *args, **kwargs):\n",
    "        super().__init__(observation_space, action_space, lr_schedule,\n",
    "                         net_arch=net_arch, activation_fn=activation_fn,\n",
    "                         ortho_init=ortho_init, *args, **kwargs)\n",
    "\n",
    "    def _build_mlp_extractor(self):\n",
    "        self.mlp_extractor = CustomNetwork(features_dim=self.features_dim,\n",
    "                                           latent_dim_pi=self.latent_dim_pi,\n",
    "                                           latent_dim_vf=self.latent_dim_vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97342e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO 방식으로 custompolicy이용해서 학습\n",
    "from stable_baselines3 import PPO\n",
    "env= Environment()\n",
    "model = PPO(\n",
    "    CustomPolicy,\n",
    "    env,\n",
    "    policy_kwargs={\"latent_dim_pi\": 128, \"latent_dim_vf\": 128},\n",
    "    verbose=2\n",
    ")\n",
    "model.learn(total_timesteps=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
